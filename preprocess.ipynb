{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Body'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mPostTypeId\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     17\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAnswerCount\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m post\u001b[39m.\u001b[39mattrib \u001b[39mand\u001b[39;00m \u001b[39mint\u001b[39m(post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mAnswerCount\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m             \u001b[39m# question\u001b[39;00m\n\u001b[0;32m     19\u001b[0m             this_question \u001b[39m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mId\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     21\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m---> 22\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m: post\u001b[39m.\u001b[39;49mattrib[\u001b[39m\"\u001b[39;49m\u001b[39mBody\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     23\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39maccepted\u001b[39m\u001b[39m\"\u001b[39m: post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mAcceptedAnswerId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAcceptedAnswerId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m post\u001b[39m.\u001b[39mattrib \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m  \n\u001b[0;32m     24\u001b[0m             }\n\u001b[0;32m     25\u001b[0m             questions[\u001b[39mint\u001b[39m(post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mId\u001b[39m\u001b[39m\"\u001b[39m])] \u001b[39m=\u001b[39m this_question\n\u001b[0;32m     26\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mint\u001b[39m(post\u001b[39m.\u001b[39mattrib[\u001b[39m\"\u001b[39m\u001b[39mPostTypeId\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m     27\u001b[0m       \u001b[39m# answer\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Body'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow_hub as tfh\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "from collections import defaultdict\n",
    "tree = ElementTree()\n",
    "tree.parse(\"data//Posts.xml\")\n",
    "root = tree.getroot()\n",
    "posts = root.iter(\"row\")\n",
    "\n",
    "questions = {}\n",
    "answers = {}\n",
    "answer_to_question_map = defaultdict(int)\n",
    "# first preprocess the data and split them into questions and answers\n",
    "for post in posts:\n",
    "      if int(post.attrib[\"PostTypeId\"]) == 1:\n",
    "            if \"AnswerCount\" in post.attrib and int(post.attrib[\"AnswerCount\"]) > 0:\n",
    "                  # question\n",
    "                  this_question = {\n",
    "                        \"id\": int(post.attrib[\"Id\"]),\n",
    "                        \"title\": post.attrib[\"Title\"],\n",
    "                        \"body\": post.attrib[\"Body\"],\n",
    "                        \"accepted\": post.attrib[\"AcceptedAnswerId\"] if \"AcceptedAnswerId\" in post.attrib else -1  \n",
    "                  }\n",
    "                  questions[int(post.attrib[\"Id\"])] = this_question\n",
    "      elif int(post.attrib[\"PostTypeId\"]) == 2:\n",
    "            # answer\n",
    "            this_answer = {\n",
    "                  \"id\": int(post.attrib[\"Id\"]),\n",
    "                  \"question_id\": post.attrib[\"ParentId\"],\n",
    "                  \"body\": post.attrib[\"Body\"],\n",
    "            }\n",
    "            answers[int(post.attrib[\"Id\"])] = this_answer\n",
    "            answer_to_question_map[int(post.attrib[\"ParentId\"])] = int(post.attrib[\"Id\"])\n",
    "# print(questions)\n",
    "# print(answers)\n",
    "\n",
    "print(f\"There are {len(questions)} questions and {len(answers)} answers\")\n",
    "print(\"Answer at id\",answer_to_question_map[questions[14][\"id\"]], \"has the answer to question at id 14\")\n",
    "print(\"Question at id 14:\", questions[14][\"body\"])\n",
    "print(\"Answer at id\", answer_to_question_map[questions[14][\"id\"]], \":\", answers[answer_to_question_map[questions[9][\"id\"]]][\"body\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question at index 9: I know that an intransitive verb accepts only one argument, i.e., the subject. So, such verbs do not need a complement. But how could one understand the concept of an intransitive complementizer, in Chomskyan syntax? After all, isn’t the very nature of complementizers to accept a complement?\n",
      "Edit: According to Andrew Radford, \"none of the English finite complementisers (e.g. if, that, that and the null finite complementiser ∅ found in main clauses) are transitive\".  So, in what sense is the complementizer that, for example, intransitive? How does it differ from other complementizers that could be classified as transitive in other languages?\n",
      "\n",
      "Answer at index 14856 : This question was based on the assumption that the transitivity of a complementizer was some inherent feature of it. One of the consequences of that feature would be that the closest nominal c-commanded by that complementizer would be given accusative case. So, if there was such kind of complementizers in English, it would be something like:\n",
      "\n",
      "*I asked if her would go to the party.\n",
      "\n",
      "But, as it turns out, that assumption is wrong. What defines a transitive complementizer is precisely the fact that it assigns accusative case to nominals, and not the other way around.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we clean the text here and print out the same question and answer before (question 9 and answer 388)\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# we parse the questions body so that it doesn't contain html tags and html symbols\n",
    "test_q = questions[14][\"body\"]\n",
    "test_a = answers[689][\"body\"]\n",
    "test_q = bs(test_q, \"lxml\").text\n",
    "test_a = bs(test_a, \"lxml\").text\n",
    "\n",
    "print(\"Question at index 9:\", test_q)\n",
    "print(\"Answer at index\", answer_to_question_map[questions[9][\"id\"]], \":\", test_a)\n",
    "# Notice \n",
    "# 1) The html tags are now gone (including <a href...>)\n",
    "# 2) HTML symbols are converted to their proper symbols &#8709 -> ∅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we clean up the text for real\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# first question\n",
    "for id, value in questions.items():\n",
    "    value[\"body\"] = bs(value[\"body\"], \"lxml\").text\n",
    "    questions[id] = value\n",
    "# then answer\n",
    "for id, value in answers.items():\n",
    "    value[\"body\"] = bs(value[\"body\"], \"lxml\").text\n",
    "    answers[id] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the (albeit citation needed) section of the Wikipedia article on aspiration:\n",
      "\n",
      "Spanish /p t k/, for example, have voice onset times (VOTs) of about 5, 10, and 30 milliseconds, whereas English /p t k/ have VOTs of about 60, 70, and 80 ms. Korean has been measured at 20, 25, and 50 ms for /p t k/ and 90, 95, and 125 for /pʰ tʰ kʰ/.\n",
      "\n",
      "This is also confirmed from my anecdotal explorations in the topic.\n",
      "The question I have is what causes the different stop consonants to have different VOTs. I couldn't find any good linguistic descriptions through some preliminary googling.\n",
      "The two hypotheses I had were both based on the (my own) idea that voicing begins once air pressure subsides from high stop-like levels to some trigger point.\n",
      "The first hypothesis was that air from a far back pressure release has to travel farther and through more obstructed corridors to reach the open air. Whereas air from a /p/ release can immediately and quickly reach the open, air from a /k/ release has to go all the way through the mouth to get outside. The latter trek is slow and results in a slower decrease of air pressure resulting in a later trigger for voicing.\n",
      "The second hypothesis was that somehow, the backer points of articulation resulted in higher pressure buildups. This would be plausible if stop bursts required some given mass of extra air to release, and this mass, distributed over the oral cavity in /p/ would be spread over a larger volume and consequently have a lower pressure than in /k/. Thereafter, even if air pressure subsided at the same rate, the higher original pressure in /k/ would take longer to hit the trigger point.\n",
      "Are there any good readings on the subject or anybody who can provide a more confident explanation?\n",
      "\n",
      "Aren't the Tibeto-Burman languages (under Sino-Tibetan) considered agglutinative?\n",
      "\n",
      "The Tibeto-Burman languages include Tibetan, Burmese, and a number of\n",
      "  other tongues, among which are the Bodo, Garo, and Lushai of Assam,\n",
      "  the Kachin of Myanmar (Burma), and perhaps also the languages of the\n",
      "  Chins and Nagas of Myanmar, the Karen tongues of Myanmar and Thailand,\n",
      "  and the Lolo of SW China. Tibeto-Burman languages are likely to be\n",
      "  tonal and have anywhere from two to six tones. They are less\n",
      "  monosyllabic and isolating than the languages of the other\n",
      "  Sino-Tibetan families. In fact, they tend to be somewhat agglutinative\n",
      "  and exhibit some degree of inflection. In an agglutinative language,\n",
      "  different linguistic elements, each of which exists separately and has\n",
      "  a fixed meaning, are joined to form one word. Affixes added to an\n",
      "  unchanged root serve as the usual method of indicating inflection in\n",
      "  the Tibeto-Burman tongues.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions[1][\"body\"]) #all clean now!\n",
    "print(answers[7][\"body\"]) #all clean now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Load the Preprocessor and Bert models, this is gonna take a while\n",
    "# we are loading the version that automatically lowercase the words for us\n",
    "BERT_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "PREPROCESS_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "preprocessor = hub.KerasLayer(PREPROCESS_URL)\n",
    "bert_model =  hub.KerasLayer(BERT_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Outputs Shape:(2, 768)\n",
      "Pooled Outputs Values:[-0.9216987  -0.39353448 -0.5393166   0.6825621   0.43848443 -0.14021152\n",
      "  0.8774711   0.26043335 -0.63112926 -0.9999658  -0.26319999  0.8510528 ]\n",
      "Sequence Outputs Shape:(2, 128, 768)\n",
      "Sequence Outputs Values:[[ 0.19451573  0.25141695  0.1907506  ... -0.24845074  0.38568527\n",
      "   0.1329099 ]\n",
      " [-0.5947868  -0.3942031   0.25245702 ... -0.769467    1.1564163\n",
      "   0.32475683]\n",
      " [ 0.00641491 -0.15766448  0.5461023  ... -0.17451026  0.60289633\n",
      "   0.4267228 ]\n",
      " ...\n",
      " [ 0.21948312 -0.20927079  0.5386832  ...  0.24693549  0.18250962\n",
      "  -0.44427082]\n",
      " [ 0.01080245 -0.4455314   0.35991004 ...  0.31722838  0.2356281\n",
      "  -0.630706  ]\n",
      " [ 0.29321173 -0.10581895  0.6114755  ...  0.20745848  0.1449465\n",
      "  -0.35353366]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the embedding \n",
    "text_test = ['this is such an amazing movie!', \"asdasdasd, asdasd\"]\n",
    "\n",
    "text_preprocessed = preprocessor(text_test)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "    \n",
    "\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "# documentation: https://github.com/spotify/annoy\n",
    "import random\n",
    "\n",
    "# helper function for encoding a sentence into bert embedding\n",
    "def encode_sentence(sentence):\n",
    "    text_preprocessed = preprocessor([sentence])\n",
    "    bert_results = bert_model(text_preprocessed)\n",
    "    return bert_results[\"pooled_output\"][0]\n",
    "\n",
    "# 768 because that's the dimension of bert \n",
    "# testing \n",
    "f = 768\n",
    "assert(encode_sentence(\"This is a test sentence\").shape == 768)\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "t.set_seed(487)\n",
    "test_list = [\"I love pizza\", \"This is a test\", \"487 is fun\"]\n",
    "for i, sentence in enumerate(test_list):\n",
    "    \n",
    "    print(i)\n",
    "    v = encode_sentence(sentence)\n",
    "    t.add_item(i, v)\n",
    "t.build(10)\n",
    "t.save(\"test.ann\")\n",
    "test_sentence = \"i LOVE BURGER!\"\n",
    "# t.load()\n",
    "k = 1\n",
    "closest_index = t.get_nns_by_vector(encode_sentence(test_sentence), 1)\n",
    "print(closest_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, sentence \u001b[39min\u001b[39;00m questions\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(index) \u001b[39m# just tracking progress since this takes hours to train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     v \u001b[39m=\u001b[39m encode_sentence(sentence[\u001b[39m\"\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      8\u001b[0m     t\u001b[39m.\u001b[39madd_item(\u001b[39mid\u001b[39m, v)\n\u001b[0;32m      9\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[95], line 8\u001b[0m, in \u001b[0;36mencode_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_sentence\u001b[39m(sentence):\n\u001b[0;32m      7\u001b[0m     text_preprocessed \u001b[39m=\u001b[39m preprocessor([sentence])\n\u001b[1;32m----> 8\u001b[0m     bert_results \u001b[39m=\u001b[39m bert_model(text_preprocessed)\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m bert_results[\u001b[39m\"\u001b[39m\u001b[39mpooled_output\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:242\u001b[0m, in \u001b[0;36mKerasLayer.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    239\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[39m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m   result \u001b[39m=\u001b[39m smart_cond\u001b[39m.\u001b[39;49msmart_cond(training,\n\u001b[0;32m    243\u001b[0m                                  \u001b[39mlambda\u001b[39;49;00m: f(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m    244\u001b[0m                                  \u001b[39mlambda\u001b[39;49;00m: f(training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[0;32m    246\u001b[0m \u001b[39m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_key:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\smart_cond.py:54\u001b[0m, in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m true_fn()\n\u001b[0;32m     53\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m false_fn()\n\u001b[0;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m   \u001b[39mreturn\u001b[39;00m control_flow_ops\u001b[39m.\u001b[39mcond(pred, true_fn\u001b[39m=\u001b[39mtrue_fn, false_fn\u001b[39m=\u001b[39mfalse_fn,\n\u001b[0;32m     57\u001b[0m                                name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:244\u001b[0m, in \u001b[0;36mKerasLayer.call.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    239\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[39m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    242\u001b[0m   result \u001b[39m=\u001b[39m smart_cond\u001b[39m.\u001b[39msmart_cond(training,\n\u001b[0;32m    243\u001b[0m                                  \u001b[39mlambda\u001b[39;00m: f(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m--> 244\u001b[0m                                  \u001b[39mlambda\u001b[39;00m: f(training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[0;32m    246\u001b[0m \u001b[39m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_key:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:704\u001b[0m, in \u001b[0;36m_call_attribute\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_attribute\u001b[39m(instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 704\u001b[0m   \u001b[39mreturn\u001b[39;00m instance\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Building our \"vocabulary\" for real now\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "t.set_seed(487)\n",
    "index = 0\n",
    "for id, sentence in questions.items():\n",
    "    print(index) # just tracking progress since this takes hours to train\n",
    "    v = encode_sentence(sentence[\"body\"])\n",
    "    t.add_item(id, v)\n",
    "    index += 1\n",
    "\n",
    "t.build(10) # build 10 trees, allegedly the more the better  \n",
    "try:\n",
    "    t.save('bert_embedding.ann')\n",
    "except OSError as e:\n",
    "    print(e) # might run out of space if this file is too large\n",
    "# this cell add the sentence bert embedding of every body in our question field \n",
    "# to annoy, we test it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed \n",
    "t.set_seed(487)\n",
    "# we need this to tell what the original sentences are \n",
    "\n",
    "t.build(10) # build 10 trees, allegedly the more the better  \n",
    "t.save('bert_embedding_unnormalize.ann')\n",
    "\n",
    "# Find k-nearest neighbor\n",
    "k = 3 # first set k = 5\n",
    "test_sentence = \"Why did Old English lose both thorn and eth?\" # one of the questions on linguistic stack exchange\n",
    "test_sentence_vector = encode_sentence(test_sentence)\n",
    "\n",
    "f = 768\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('bert_embedding.ann') \n",
    "nearest_i = u.get_nns_by_vector(test_sentence_vector, k) # will return the index of the k nearest neighbor\n",
    "nearest_word = [(i, questions[i]) for i in nearest_i] # \n",
    "nearest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
