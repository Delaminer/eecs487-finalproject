{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:51:07.233702: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-31 11:51:07.577162: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 11:51:08.958234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 117656 questions and 286657 answers\n",
      "Answer at id 450027 has the answer to question at id 14\n",
      "Question at id 14: <p>Are there any concrete rules that say which words (parts of speech) in a title should start with a capital letter? What would be a correct capitalization for the title of this question?</p>\n",
      "\n",
      "Answer at id 450027 : <h1>StackExchange \"Questions\"</h1>\n",
      "\n",
      "<p>A \"title\" isn't typically a sentence, nor a question. Because you're merely presenting a question, special rules for capitalization should not apply.</p>\n",
      "\n",
      "<p>Traditionally <a href=\"https://www.etymonline.com/word/title\" rel=\"nofollow noreferrer\">a title was,</a></p>\n",
      "\n",
      "<blockquote>\n",
      "  <p>c. 1300, <strong>\"inscription, heading,\" from Old French title \"title or chapter of a book; position; legal permit\"</strong> (12c., Modern French titre, by dissimilation), and in part from Old English titul, both from Latin titulus <strong>\"inscription, label, ticket, placard, heading; honorable appellation, title of honor,\"</strong> of unknown origin. Meaning <strong>\"name of a book, play, etc.\"</strong> first recorded mid-14c. The sense of \"name showing a person's rank\" in English is first attested 1580s. Sports championship sense attested from 1913 (originally in lawn tennis), hence titlist (1913).</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>None of that would cover a complete and independent interrogative.</p>\n",
      "\n",
      "<p>I would argue if your question is, <em>\"Which words in a title should be capitalized?</em>\" the correct capitalization is <strong>exactly that</strong>. If you wanted to title it, <strong>\"Capitalization in a Title\"</strong> you could however capitalize the the first and last words. Stack Overflow \"questions\" are highlighted for search engine optimization and head the page. But they're not \"titles\" in any sense deserving of special rules.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow_hub as tfh\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "from collections import defaultdict\n",
    "tree = ElementTree()\n",
    "tree.parse(\"data//Posts.xml\")\n",
    "root = tree.getroot()\n",
    "posts = root.iter(\"row\")\n",
    "\n",
    "questions = {}\n",
    "answers = {}\n",
    "answer_to_question_map = defaultdict(int)\n",
    "# first preprocess the data and split them into questions and answers\n",
    "for post in posts:\n",
    "      if int(post.attrib[\"PostTypeId\"]) == 1:\n",
    "            if \"AnswerCount\" in post.attrib and int(post.attrib[\"AnswerCount\"]) > 0:\n",
    "                  # question\n",
    "                  this_question = {\n",
    "                        \"id\": int(post.attrib[\"Id\"]),\n",
    "                        \"title\": post.attrib[\"Title\"],\n",
    "                        \"body\": post.attrib[\"Body\"],\n",
    "                        \"accepted\": post.attrib[\"AcceptedAnswerId\"] if \"AcceptedAnswerId\" in post.attrib else -1  \n",
    "                  }\n",
    "                  questions[int(post.attrib[\"Id\"])] = this_question\n",
    "      elif int(post.attrib[\"PostTypeId\"]) == 2:\n",
    "            # answer\n",
    "            this_answer = {\n",
    "                  \"id\": int(post.attrib[\"Id\"]),\n",
    "                  \"question_id\": post.attrib[\"ParentId\"],\n",
    "                  \"body\": post.attrib[\"Body\"],\n",
    "            }\n",
    "            answers[int(post.attrib[\"Id\"])] = this_answer\n",
    "            answer_to_question_map[int(post.attrib[\"ParentId\"])] = int(post.attrib[\"Id\"])\n",
    "# print(questions)\n",
    "# print(answers)\n",
    "\n",
    "print(f\"There are {len(questions)} questions and {len(answers)} answers\")\n",
    "print(\"Answer at id\",answer_to_question_map[questions[14][\"id\"]], \"has the answer to question at id 14\")\n",
    "print(\"Question at id 14:\", questions[14][\"body\"])\n",
    "print(\"Answer at id\", answer_to_question_map[questions[14][\"id\"]], \":\", answers[answer_to_question_map[questions[14][\"id\"]]][\"body\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question at index 9: Are there any concrete rules that say which words (parts of speech) in a title should start with a capital letter? What would be a correct capitalization for the title of this question?\n",
      "\n",
      "Answer at index 450027 : StackExchange \"Questions\"\n",
      "A \"title\" isn't typically a sentence, nor a question. Because you're merely presenting a question, special rules for capitalization should not apply.\n",
      "Traditionally a title was,\n",
      "\n",
      "c. 1300, \"inscription, heading,\" from Old French title \"title or chapter of a book; position; legal permit\" (12c., Modern French titre, by dissimilation), and in part from Old English titul, both from Latin titulus \"inscription, label, ticket, placard, heading; honorable appellation, title of honor,\" of unknown origin. Meaning \"name of a book, play, etc.\" first recorded mid-14c. The sense of \"name showing a person's rank\" in English is first attested 1580s. Sports championship sense attested from 1913 (originally in lawn tennis), hence titlist (1913).\n",
      "\n",
      "None of that would cover a complete and independent interrogative.\n",
      "I would argue if your question is, \"Which words in a title should be capitalized?\" the correct capitalization is exactly that. If you wanted to title it, \"Capitalization in a Title\" you could however capitalize the the first and last words. Stack Overflow \"questions\" are highlighted for search engine optimization and head the page. But they're not \"titles\" in any sense deserving of special rules.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we clean the text here and print out the same question and answer before (question 9 and answer 388)\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# we parse the questions body so that it doesn't contain html tags and html symbols\n",
    "test_q = questions[14][\"body\"]\n",
    "answer_id = answer_to_question_map[questions[14][\"id\"]]\n",
    "test_a = answers[answer_id][\"body\"]\n",
    "test_q = bs(test_q, \"lxml\").text\n",
    "test_a = bs(test_a, \"lxml\").text\n",
    "\n",
    "print(\"Question at index 9:\", test_q)\n",
    "print(\"Answer at index\", answer_to_question_map[questions[14][\"id\"]], \":\", test_a)\n",
    "# Notice \n",
    "# 1) The html tags are now gone (including <a href...>)\n",
    "# 2) HTML symbols are converted to their proper symbols &#8709 -> âˆ…"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_860/481259403.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  value[\"title\"] = bs(value[\"title\"], \"lxml\").text\n"
     ]
    }
   ],
>>>>>>> 055996ac3eb9c035150c64bc308412bdf082f325
   "source": [
    "# Now we clean up the text for real\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# print(answers[5])\n",
    "# first question\n",
    "for id, value in questions.items():\n",
    "    value[\"title\"] = bs(value[\"title\"], \"lxml\").text\n",
    "    value[\"body\"] = bs(value[\"body\"], \"lxml\").text\n",
    "    questions[id] = value\n",
    "# then answer\n",
    "for id, value in answers.items():\n",
    "    # print(value)\n",
    "    # value[\"title\"] = bs(value[\"title\"], \"lxml\").text\n",
    "    value[\"body\"] = bs(value[\"body\"], \"lxml\").text\n",
    "    answers[id] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When should I use can? When should I use could?\n",
      "What is right under what context?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(questions[\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m#all clean now!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(answers[\u001b[39m7\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m#all clean now!\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 7"
     ]
    }
   ],
   "source": [
    "# print(questions[1][\"body\"]) #all clean now!\n",
    "# print(answers[7][\"body\"]) #all clean now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. 'C:\\Users\\user\\AppData\\Local\\Temp\\tfhub_modules\\602d30248ff7929470db09f7385fc895e9ceb4c0' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m BERT_URL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m PREPROCESS_URL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m preprocessor \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39;49mKerasLayer(PREPROCESS_URL)\n\u001b[0;32m     10\u001b[0m bert_model \u001b[39m=\u001b[39m  hub\u001b[39m.\u001b[39mKerasLayer(BERT_URL)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:157\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_shape \u001b[39m=\u001b[39m data_structures\u001b[39m.\u001b[39mNoDependency(\n\u001b[0;32m    154\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_options \u001b[39m=\u001b[39m load_options\n\u001b[1;32m--> 157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func \u001b[39m=\u001b[39m load_module(handle, tags, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_options)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_hub_module_v1 \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func, \u001b[39m\"\u001b[39m\u001b[39m_is_hub_module_v1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[39m# Update with the defaults when using legacy TF1 Hub format.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:459\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# Expected before TF2.4.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m       set_load_options \u001b[39m=\u001b[39m load_options\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m module_v2\u001b[39m.\u001b[39;49mload(handle, tags\u001b[39m=\u001b[39;49mtags, options\u001b[39m=\u001b[39;49mset_load_options)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow_hub\\module_v2.py:107\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    102\u001b[0m saved_model_pbtxt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m    103\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(module_path),\n\u001b[0;32m    104\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(saved_model_path) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[1;32m--> 107\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m contains neither \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m nor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    109\u001b[0m                    (module_path, tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[0;32m    110\u001b[0m                     tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m options:\n\u001b[0;32m    113\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mgetattr\u001b[39m(tf, \u001b[39m\"\u001b[39m\u001b[39msaved_model\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39mLoadOptions\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. 'C:\\Users\\user\\AppData\\Local\\Temp\\tfhub_modules\\602d30248ff7929470db09f7385fc895e9ceb4c0' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:55:04.048581: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.048994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.049083: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.049373: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.049614: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.049702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.049760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.115366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.115987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.116128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.116928: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.117073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.117131: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.117201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.210318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.210914: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.211019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.211385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.211541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.211664: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.211839: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.452430: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.453440: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.453683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.454738: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.455082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.455164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n",
      "2023-03-31 11:55:04.455231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'seq_length' with dtype int32\n",
      "\t [[{{node seq_length}}]]\n"
>>>>>>> 055996ac3eb9c035150c64bc308412bdf082f325
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Load the Preprocessor and Bert models, this is gonna take a while\n",
    "# we are loading the version that automatically lowercase the words for us\n",
    "BERT_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "PREPROCESS_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "preprocessor = hub.KerasLayer(PREPROCESS_URL)\n",
    "bert_model =  hub.KerasLayer(BERT_URL)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Testing the embedding \u001b[39;00m\n\u001b[0;32m      2\u001b[0m text_test \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mthis is such an amazing movie!\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39masdasdasd, asdasd\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m text_preprocessed \u001b[39m=\u001b[39m preprocessor(text_test)\n\u001b[0;32m      5\u001b[0m bert_results \u001b[39m=\u001b[39m bert_model(text_preprocessed)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPooled Outputs Shape:\u001b[39m\u001b[39m{\u001b[39;00mbert_results[\u001b[39m\"\u001b[39m\u001b[39mpooled_output\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Outputs Shape:(2, 768)\n",
      "Pooled Outputs Values:[-0.92169887 -0.39353454 -0.5393167   0.68256235  0.43848473 -0.1402117\n",
      "  0.8774715   0.2604335  -0.63112926 -0.9999657  -0.26320004  0.851053  ]\n",
      "Sequence Outputs Shape:(2, 128, 768)\n",
      "Sequence Outputs Values:[[ 0.1945158   0.25141707  0.19075048 ... -0.24845102  0.38568562\n",
      "   0.13291012]\n",
      " [-0.5947866  -0.3942027   0.25245667 ... -0.7694676   1.1564162\n",
      "   0.32475698]\n",
      " [ 0.00641498 -0.15766431  0.54610246 ... -0.17451052  0.60289645\n",
      "   0.42672253]\n",
      " ...\n",
      " [ 0.21948308 -0.20927092  0.5386828  ...  0.24693584  0.18250984\n",
      "  -0.44427103]\n",
      " [ 0.01080255 -0.44553187  0.35990942 ...  0.31722796  0.23562792\n",
      "  -0.6307055 ]\n",
      " [ 0.29321137 -0.10581908  0.6114752  ...  0.20745826  0.14494672\n",
      "  -0.35353372]]\n"
>>>>>>> 055996ac3eb9c035150c64bc308412bdf082f325
     ]
    }
   ],
   "source": [
    "# Testing the embedding \n",
    "text_test = ['this is such an amazing movie!', \"asdasdasd, asdasd\"]\n",
    "\n",
    "text_preprocessed = preprocessor(text_test)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "# documentation: https://github.com/spotify/annoy\n",
    "import random\n",
    "\n",
    "# helper function for encoding a sentence into bert embedding\n",
    "def encode_sentence(sentence):\n",
    "    text_preprocessed = preprocessor([sentence])\n",
    "    bert_results = bert_model(text_preprocessed)\n",
    "    return bert_results[\"pooled_output\"][0]\n",
    "\n",
    "# 768 because that's the dimension of bert \n",
    "# testing \n",
    "f = 768\n",
    "assert(encode_sentence(\"This is a test sentence\").shape == 768)\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "t.set_seed(487)\n",
    "test_list = [\"I love pizza\", \"This is a test\", \"487 is fun\"]\n",
    "for i, sentence in enumerate(test_list):\n",
    "    \n",
    "    print(i)\n",
    "    v = encode_sentence(sentence)\n",
    "    t.add_item(i, v)\n",
    "t.build(10)\n",
    "t.save(\"test.ann\")\n",
    "test_sentence = \"i LOVE BURGER!\"\n",
    "# t.load()\n",
    "k = 1\n",
    "closest_index = t.get_nns_by_vector(encode_sentence(test_sentence), 1)\n",
    "print(closest_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n",
      "14\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list = [sentence for _, sentence in questions.items()]\n",
    "qs = question_list[:10]\n",
    "[print(question['id']) for question in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0, item 0 (id 1)\n",
      "Block 0, item 1 (id 2)\n",
      "Block 0, item 2 (id 3)\n",
      "Block 0, item 3 (id 6)\n",
      "Block 0, item 4 (id 7)\n",
      "Block 0, item 5 (id 8)\n",
      "Block 0, item 6 (id 14)\n",
      "Block 0, item 7 (id 16)\n",
      "Block 0, item 8 (id 17)\n",
      "Block 0, item 9 (id 18)\n",
      "Block 0, item 10 (id 19)\n",
      "Block 0, item 11 (id 21)\n",
      "Block 0, item 12 (id 23)\n",
      "Block 0, item 13 (id 45)\n",
      "Block 0, item 14 (id 48)\n",
      "Block 0, item 15 (id 50)\n",
      "Block 0, item 16 (id 56)\n",
      "Block 0, item 17 (id 57)\n",
      "Block 0, item 18 (id 66)\n",
      "Block 0, item 19 (id 78)\n",
      "79\n",
      "Finished block 0\n",
      "Block 1, item 20 (id 79)\n",
      "Block 1, item 21 (id 92)\n",
      "Block 1, item 22 (id 97)\n",
      "Block 1, item 23 (id 105)\n",
      "Block 1, item 24 (id 114)\n",
      "Block 1, item 25 (id 118)\n",
      "Block 1, item 26 (id 124)\n",
      "Block 1, item 27 (id 125)\n",
      "Block 1, item 28 (id 128)\n",
      "Block 1, item 29 (id 135)\n",
      "Block 1, item 30 (id 138)\n",
      "Block 1, item 31 (id 152)\n",
      "Block 1, item 32 (id 154)\n",
      "Block 1, item 33 (id 158)\n",
      "Block 1, item 34 (id 165)\n",
      "Block 1, item 35 (id 166)\n",
      "Block 1, item 36 (id 170)\n",
      "Block 1, item 37 (id 172)\n",
      "Block 1, item 38 (id 177)\n",
      "Block 1, item 39 (id 178)\n",
      "179\n",
      "Finished block 1\n",
      "Block 2, item 40 (id 179)\n",
      "Block 2, item 41 (id 183)\n",
      "Block 2, item 42 (id 184)\n",
      "Block 2, item 43 (id 189)\n",
      "Block 2, item 44 (id 192)\n",
      "Block 2, item 45 (id 200)\n",
      "Block 2, item 46 (id 204)\n",
      "Block 2, item 47 (id 205)\n",
      "Block 2, item 48 (id 211)\n",
      "Block 2, item 49 (id 215)\n",
      "Block 2, item 50 (id 224)\n",
      "Block 2, item 51 (id 225)\n",
      "Block 2, item 52 (id 234)\n",
      "Block 2, item 53 (id 235)\n",
      "Block 2, item 54 (id 243)\n",
      "Block 2, item 55 (id 246)\n",
      "Block 2, item 56 (id 251)\n",
      "Block 2, item 57 (id 256)\n",
      "Block 2, item 58 (id 258)\n",
      "Block 2, item 59 (id 263)\n",
      "264\n",
      "Finished block 2\n",
      "Block 3, item 60 (id 264)\n",
      "Block 3, item 61 (id 270)\n",
      "Block 3, item 62 (id 274)\n",
      "Block 3, item 63 (id 282)\n",
      "Block 3, item 64 (id 286)\n",
      "Block 3, item 65 (id 292)\n",
      "Block 3, item 66 (id 294)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m encode_title_and_body \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m sentence: (encode_sentence(sentence[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m+\u001b[39m encode_sentence(sentence[\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m])) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39m# choose 1 below \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# build_annoy(\"bert_embedding_body\", encode_body, batch_size=1000) \u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m build_annoy(\u001b[39m\"\u001b[39;49m\u001b[39mbert_embedding_title\u001b[39;49m\u001b[39m\"\u001b[39;49m, encode_title, batch_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m) \n\u001b[1;32m     39\u001b[0m \u001b[39m# build_annoy(\"bert_embedding_title_and_body\", encode_title_and_body, batch_size=1000) \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m, in \u001b[0;36mbuild_annoy\u001b[0;34m(name, custom_encode_sentence, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m num_blocks \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(num_items \u001b[39m/\u001b[39m batch_size)\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_blocks):\n\u001b[0;32m---> 30\u001b[0m     build_block(block)\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mbuild_annoy.<locals>.build_block\u001b[0;34m(block)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBlock \u001b[39m\u001b[39m{\u001b[39;00mblock\u001b[39m}\u001b[39;00m\u001b[39m, item \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m (id \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# just tracking progress since this takes hours to train\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     v \u001b[39m=\u001b[39m custom_encode_sentence(sentence)\n\u001b[0;32m---> 16\u001b[0m     t\u001b[39m.\u001b[39;49madd_item(\u001b[39mid\u001b[39;49m, v)\n\u001b[1;32m     17\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     19\u001b[0m t\u001b[39m.\u001b[39mbuild(\u001b[39m10\u001b[39m) \u001b[39m# build 100 trees, allegedly the more the better  \u001b[39;00m\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1089\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\n\u001b[1;32m   1085\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1086\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrided_slice\u001b[39m\u001b[39m\"\u001b[39m, [tensor] \u001b[39m+\u001b[39m begin \u001b[39m+\u001b[39m end \u001b[39m+\u001b[39m strides,\n\u001b[1;32m   1087\u001b[0m     skip_on_eager\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m   1088\u001b[0m   \u001b[39mif\u001b[39;00m begin:\n\u001b[0;32m-> 1089\u001b[0m     packed_begin, packed_end, packed_strides \u001b[39m=\u001b[39m (stack(begin), stack(end),\n\u001b[1;32m   1090\u001b[0m                                                 stack(strides))\n\u001b[1;32m   1091\u001b[0m     \u001b[39m# TODO(mdan): Instead of implicitly casting, it's better to enforce the\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# same dtypes.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mif\u001b[39;00m (packed_begin\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         packed_end\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         packed_strides\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64):\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1484\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1482\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1483\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1485\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m   1486\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 344\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/class/eecs487/eecs487-finalproject/.venv/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# build annoy with body only\n",
    "def build_annoy(name, custom_encode_sentence, batch_size=1000):\n",
    "    def build_block(block):\n",
    "        def in_range(i):\n",
    "            return i >= block * batch_size and i < (block+1) * batch_size\n",
    "        \n",
    "        t = AnnoyIndex(f, 'angular')\n",
    "        t.set_seed(487)\n",
    "        index = 0\n",
    "        for id, sentence in questions.items():\n",
    "            if not in_range(index):\n",
    "                index += 1\n",
    "                continue\n",
    "            print(f\"Block {block}, item {index} (id {id})\") # just tracking progress since this takes hours to train\n",
    "            v = custom_encode_sentence(sentence)\n",
    "            t.add_item(id, v)\n",
    "            index += 1\n",
    "\n",
    "        t.build(10) # build 100 trees, allegedly the more the better  \n",
    "        try:\n",
    "            print(t.get_n_items())\n",
    "            t.save(f'annoy/blocks/{name}_block{block}.ann')\n",
    "        except OSError as e:\n",
    "            print(e) # might run out of space if this file is too large\n",
    "        print(f\"Finished block {block}\")\n",
    "\n",
    "    num_items = len(questions.items())\n",
    "    num_blocks = int(num_items / batch_size)\n",
    "    for block in range(num_blocks):\n",
    "        build_block(block)\n",
    "\n",
    "encode_body = lambda sentence: encode_sentence(sentence[\"body\"])\n",
    "encode_title = lambda sentence: encode_sentence(sentence[\"title\"])\n",
    "encode_title_and_body = lambda sentence: (encode_sentence(sentence[\"title\"]) + encode_sentence(sentence[\"body\"])) / 2\n",
    "\n",
    "# choose 1 below \n",
    "# build_annoy(\"bert_embedding_body\", encode_body, batch_size=1000) \n",
    "build_annoy(\"bert_embedding_title\", encode_title, batch_size=20) \n",
    "# build_annoy(\"bert_embedding_title_and_body\", encode_title_and_body, batch_size=1000) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "179\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "# Inspecting Annoy files\n",
    "# How can we merge blocks together?\n",
    "blocks = []\n",
    "for i in range(3):\n",
    "    name = f\"annoy/blocks/bert_embedding_title_block{i}.ann\"\n",
    "    a = AnnoyIndex(f, 'angular')\n",
    "    a.load(name)\n",
    "    print(a.get_n_items())\n",
    "    blocks.append(a)\n",
    "\n",
    "# Make a new model\n",
    "f = 768\n",
    "merged = AnnoyIndex(f, 'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(236,\n",
       "  {'id': 236,\n",
       "   'title': 'Verb-attraction parameter in Portuguese',\n",
       "   'body': 'Prof. John McWorther, in his course on Linguistics, said, in a lecture about principles and parameters: \"if a language is pro-drop, the verb attraction parameter is always set on. If a language isn\\'t pro-drop, then it could go either way.\" Using his examples:\\n\\nJohn often kisses Mary (English, pro-drop: off, verb-attraction: off)\\nJean embrasse souvent Marie (French, pro-drop: off, verb-attraction: on)\\nBeso normalmente Maria (Spanish, pro-drop: on, verb-attraction: on)\\n\\nThe examples above show that in English, the verb comes after the adverb, while in French, the verb comes before the adverb (it has moved from its original V position to the T position up in the tree). In Spanish, which is a pro-drop language, the verb has to come before the adverb, as in French. Now, consider the same example in Portuguese:\\n\\nNormalmente beijo a Maria (Portuguese, pro-drop: on, verb-attraction: off)\\n\\nThis piece of evidence seems to contradict the hypothesis above. In Portuguese, the subject pronoun can be omitted (pro-drop), but the verb does not have to move (although you could also say \"beijo normalmente a Maria\"). How can this be explained?\\n',\n",
       "   'accepted': '238'}),\n",
       " (1264,\n",
       "  {'id': 1264,\n",
       "   'title': 'Syllable counting acronyms and abbreviations',\n",
       "   'body': 'This question is similar to one I asked earlier about \"Numbers as words\".\\nMany readability stats call for counting syllables.  The direction for abbreviations and acronyms seems to be to formulate the syllable count as if though the word(s) were not abbreviated (correct me if I\\'m wrong on this).  So \"DVD\" would become digital-video-device and count for 9 syllables.  And 7 \"pm\" becomes \"Post-Meridiem\" or 4 syllables.  This does not make sense to me as we don\\'t comprehend pm as post-meridiem or DVD as digital video device.  These acronyms/abbreviations have become words unto themselves.\\nI\\'m confused as to how i should proceed with counting these type of words and am looking for clarification on how to treat these words for readability statistics. \\n',\n",
       "   'accepted': '1265'}),\n",
       " (1685,\n",
       "  {'id': 1685,\n",
       "   'title': 'Distinction between definiteness and specificity',\n",
       "   'body': 'May I have an example of a language which separately marks definiteness and specificity (or indefiniteness and non-specificity), and also a principled way for deciding which of the two sets of terms is appropriate in description?\\n',\n",
       "   'accepted': '1686'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed \n",
    "# t.set_seed(487)\n",
    "# we need this to tell what the original sentences are \n",
    "\n",
    "# t.build(10) # build 10 trees, allegedly the more the better  \n",
    "# t.save('bert_embedding_unnormalize.ann')\n",
    "\n",
    "# Find k-nearest neighbor\n",
    "k = 3 # first set k = 5\n",
    "test_sentence = \"Why did Old English lose both thorn and eth?\" # one of the questions on linguistic stack exchange\n",
    "test_sentence_vector = encode_sentence(test_sentence)\n",
    "\n",
    "f = 768\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('bert_embedding1.ann') \n",
    "nearest_i = u.get_nns_by_vector(test_sentence_vector, k) # will return the index of the k nearest neighbor\n",
    "nearest_word = [(i, questions[i]) for i in nearest_i] # \n",
    "nearest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
