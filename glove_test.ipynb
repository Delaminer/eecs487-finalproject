{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "from typing import List\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_embed():\n",
    "    try:\n",
    "        embed = KeyedVectors.load(\"word2vec200.kv\")\n",
    "    except:\n",
    "        embed = gensim.downloader.load('glove-wiki-gigaword-200')\n",
    "        embed.save(\"word2vec200.kv\")\n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = get_embed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[\"hi\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_db():\n",
    "    with open(\"words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        data = data.lower()\n",
    "        data = word_tokenize(data)\n",
    "        tokens = []\n",
    "        for token in data:\n",
    "            if token in embed:\n",
    "                tokens.append(token)\n",
    "        return list(tokens)\n",
    "\n",
    "words = get_word_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'annoy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mannoy\u001b[39;00m \u001b[39mimport\u001b[39;00m AnnoyIndex\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m f \u001b[39m=\u001b[39m embed[words[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]  \u001b[39m# Length of item vector that will be indexed\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'annoy'"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "\n",
    "f = embed[words[0]].shape[0]  # Length of item vector that will be indexed\n",
    "\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "for i, word in enumerate(words):\n",
    "    v = embed[word]\n",
    "    t.add_item(i, v)\n",
    "\n",
    "index_to_word = dict(enumerate(words))\n",
    "word_to_index = dict([(word, i) for i, word in enumerate(words)])\n",
    "\n",
    "t.build(10) # 10 trees\n",
    "t.save('test.ann')\n",
    "\n",
    "# ...\n",
    "\n",
    "word = \"machine\"\n",
    "this_word_index = word_to_index[word]\n",
    "\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "nearest_i = u.get_nns_by_item(this_word_index, 1000) # will find the 1000 nearest neighbors\n",
    "nearest_word = [(i, index_to_word[i]) for i in nearest_i]\n",
    "nearest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
