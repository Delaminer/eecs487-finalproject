{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['c:\\\\Users\\\\user\\\\Desktop\\\\WN 22\\\\EECS 487\\\\eecs487-finalproject\\\\model', 'c:\\\\Python310\\\\python310.zip', 'c:\\\\Python310\\\\DLLs', 'c:\\\\Python310\\\\lib', 'c:\\\\Python310', '', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\Pythonwin', 'c:\\\\Python310\\\\lib\\\\site-packages', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import DataLoader, FineTunedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\user\\\\Desktop\\\\WN 22\\\\EECS 487\\\\eecs487-finalproject\\\\model', 'c:\\\\Python310\\\\python310.zip', 'c:\\\\Python310\\\\DLLs', 'c:\\\\Python310\\\\lib', 'c:\\\\Python310', '', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\Pythonwin', 'c:\\\\Python310\\\\lib\\\\site-packages', '..', '..', '..', '..']\n",
      "Found saved data\n",
      "Successfully load pickle files\n",
      "Successfully load unique_titles\n",
      "Successfully load titles_id\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = DataLoader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test, and validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split train as 50%, val as 25%, test as 25% \n",
    "def get_training_splits(data):\n",
    "    X_train, X_test = train_test_split(\n",
    "        data, test_size = 0.25, random_state=487, shuffle=True \n",
    "    )\n",
    "\n",
    "    X_train, X_val = train_test_split(\n",
    "        X_train, test_size = 0.33, random_state=487, shuffle=True\n",
    "    )\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "X_train, X_val, X_test = get_training_splits(data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q1_embedding': tensor([-0.6612, -0.4439, -0.9686,  0.7238,  0.7167, -0.2222,  0.7024,  0.2712,\n",
      "        -0.8396, -0.9999, -0.4180,  0.9553,  0.9165,  0.8304,  0.6922, -0.7643,\n",
      "        -0.1987, -0.6001,  0.2495,  0.2315,  0.6463,  1.0000, -0.2955,  0.3421,\n",
      "         0.2795,  0.9798, -0.7503,  0.7382,  0.8752,  0.6916, -0.4653,  0.1631,\n",
      "        -0.9689, -0.1673, -0.9767, -0.9771,  0.4652, -0.3426, -0.0740,  0.1112,\n",
      "        -0.6656,  0.2647,  1.0000, -0.4610,  0.6262, -0.2689, -1.0000,  0.2466,\n",
      "        -0.6560,  0.9377,  0.9460,  0.9306,  0.2956,  0.4798,  0.5133, -0.4116,\n",
      "        -0.2058,  0.1331, -0.3158, -0.5013, -0.6846,  0.5949, -0.9109, -0.6035,\n",
      "         0.9541,  0.9584, -0.2111, -0.3495, -0.1310, -0.0682,  0.7610,  0.1285,\n",
      "        -0.4351, -0.8127,  0.7524,  0.2371, -0.7245,  1.0000, -0.5208, -0.9036,\n",
      "         0.9678,  0.9117,  0.6605, -0.6597,  0.8889, -1.0000,  0.5640, -0.1380,\n",
      "        -0.9508,  0.2594,  0.5715, -0.1929,  0.9742,  0.6347, -0.8308, -0.5404,\n",
      "        -0.1857, -0.9296, -0.2791, -0.5161,  0.1527, -0.1671, -0.3983, -0.3340,\n",
      "         0.3879, -0.5577, -0.3064,  0.8177,  0.5468,  0.6408,  0.6190, -0.3729,\n",
      "         0.4194, -0.8579,  0.5019, -0.3764, -0.9572, -0.6340, -0.9433,  0.5756,\n",
      "        -0.5308, -0.2993,  0.7407, -0.7847,  0.4744, -0.1986, -0.9485, -1.0000,\n",
      "        -0.5573, -0.8464, -0.4885, -0.2959, -0.9135, -0.9105,  0.6413,  0.8414,\n",
      "         0.3578,  0.9998, -0.2586,  0.8377, -0.3002, -0.8225,  0.6906, -0.4886,\n",
      "         0.9107, -0.0217, -0.3153,  0.2239, -0.5694,  0.0572, -0.8441, -0.1119,\n",
      "        -0.9173, -0.7570, -0.2237,  0.7908, -0.6597, -0.9697, -0.5627, -0.0618,\n",
      "        -0.4180,  0.6828,  0.8658,  0.3808, -0.7577,  0.4559,  0.6418,  0.4667,\n",
      "        -0.7380, -0.0648,  0.2613, -0.4065, -0.9780, -0.9048, -0.2496,  0.3034,\n",
      "         0.9549,  0.6782,  0.2130,  0.6879, -0.3541,  0.4614, -0.9029,  0.9303,\n",
      "        -0.0923,  0.2195, -0.8072,  0.6964, -0.5635,  0.6346,  0.6428, -0.5379,\n",
      "        -0.4783, -0.1468, -0.5187, -0.4005, -0.9125,  0.0992, -0.1016, -0.3403,\n",
      "        -0.0931,  0.8228,  0.8901,  0.3583,  0.6590,  0.5832, -0.5070, -0.3908,\n",
      "         0.0285,  0.1615, -0.0029,  0.9508, -0.9270,  0.0782, -0.7376, -0.9211,\n",
      "        -0.1345, -0.6227, -0.2566, -0.7255,  0.7009, -0.5330,  0.4947,  0.3942,\n",
      "        -0.5859, -0.7374,  0.3881, -0.4901,  0.5090, -0.2568,  0.9763,  0.9772,\n",
      "        -0.5877,  0.0741,  0.9356, -0.9820, -0.5665,  0.4301, -0.2603,  0.7923,\n",
      "        -0.5729,  0.9753,  0.9415,  0.6781, -0.5194, -0.9437, -0.4578, -0.8474,\n",
      "        -0.0658,  0.7092,  0.9569,  0.7358,  0.3993, -0.4647, -0.4480,  0.9166,\n",
      "        -0.9896, -0.8945, -0.7647, -0.1186, -0.9614,  0.9292,  0.3516,  0.8536,\n",
      "        -0.4854, -0.7029, -0.8503,  0.4677,  0.1046,  0.9348, -0.5671, -0.7720,\n",
      "        -0.6747, -0.8068, -0.0998, -0.2187, -0.7452,  0.0103, -0.7655,  0.6074,\n",
      "         0.5423,  0.4264, -0.9639,  0.9829,  1.0000,  0.9165,  0.7426,  0.2540,\n",
      "        -1.0000, -0.7836,  1.0000, -0.9945, -1.0000, -0.7065, -0.5872,  0.2843,\n",
      "        -1.0000, -0.2788,  0.0912, -0.5891,  0.8454,  0.8672,  0.8571, -1.0000,\n",
      "         0.5518,  0.6350, -0.7356,  0.9595, -0.4659,  0.8748,  0.7660,  0.5872,\n",
      "        -0.1922,  0.4533, -0.9885, -0.7257, -0.8155, -0.9333,  0.9998,  0.2043,\n",
      "        -0.8420, -0.5182,  0.6464, -0.1664, -0.0898, -0.8879, -0.2871,  0.4393,\n",
      "         0.7582,  0.2406,  0.4308, -0.3686,  0.3350,  0.7118, -0.0077,  0.6793,\n",
      "        -0.8474, -0.2552,  0.0402,  0.0781, -0.8701, -0.9279,  0.8015, -0.2731,\n",
      "         0.8289,  1.0000,  0.6494, -0.4610,  0.7693,  0.1433, -0.8563,  1.0000,\n",
      "         0.9064, -0.9257, -0.6474,  0.7763, -0.5206, -0.6246,  0.9970, -0.1333,\n",
      "        -0.8813, -0.5169,  0.9584, -0.9599,  0.9995, -0.6556, -0.8890,  0.8355,\n",
      "         0.7930, -0.6801, -0.5559,  0.0975, -0.9133,  0.4367, -0.7081,  0.6303,\n",
      "         0.3584, -0.0412,  0.7127, -0.2736, -0.6503,  0.2199, -0.8316, -0.1921,\n",
      "         0.9847,  0.4588, -0.2917,  0.0186, -0.2453, -0.8417, -0.8686,  0.7187,\n",
      "         1.0000, -0.3598,  0.9283, -0.6062, -0.0549,  0.0614,  0.6049,  0.6307,\n",
      "        -0.2905, -0.7516,  0.8254, -0.7918, -0.9675,  0.3536,  0.1572, -0.4266,\n",
      "         1.0000,  0.6305,  0.3049,  0.2459,  0.9845,  0.1871,  0.2409,  0.9524,\n",
      "         0.9468, -0.2592,  0.6980,  0.4333, -0.9483, -0.3367, -0.6158, -0.0114,\n",
      "        -0.9130,  0.1183, -0.8397,  0.8866,  0.9561,  0.3252,  0.3417,  0.8446,\n",
      "         1.0000, -0.9538,  0.4308,  0.5869,  0.3592, -1.0000, -0.5032, -0.3595,\n",
      "        -0.1971, -0.9492, -0.4905,  0.2905, -0.8817,  0.9159,  0.7633, -0.8889,\n",
      "        -0.9282, -0.5957,  0.6795, -0.0608, -0.9962, -0.5868, -0.3205,  0.5714,\n",
      "        -0.3844, -0.6379, -0.4009, -0.2648,  0.3499, -0.1419,  0.6670,  0.9680,\n",
      "         0.3338, -0.9117, -0.4687, -0.2146, -0.6743,  0.8085, -0.7036, -0.9641,\n",
      "        -0.2416,  1.0000, -0.7280,  0.9764,  0.4789,  0.4018, -0.2482,  0.3567,\n",
      "         0.9873,  0.4735, -0.9494, -0.9121,  0.5829, -0.4542,  0.7081,  0.8853,\n",
      "         0.9408,  0.6250,  0.8545,  0.1920, -0.1070,  0.0826,  0.9673, -0.0604,\n",
      "        -0.1453, -0.3395, -0.1278, -0.3542,  0.2623,  1.0000,  0.2101,  0.5992,\n",
      "        -0.9549, -0.8849, -0.6661,  1.0000,  0.7353, -0.1623,  0.7203,  0.5872,\n",
      "        -0.0774,  0.4207, -0.2187, -0.1811,  0.2737,  0.0035,  0.7982, -0.6248,\n",
      "        -0.9190, -0.6022,  0.2998, -0.8292,  1.0000, -0.5142, -0.3300, -0.3804,\n",
      "        -0.1705, -0.7678, -0.0877, -0.9015, -0.1640,  0.2316,  0.7863,  0.2538,\n",
      "        -0.6561, -0.6618,  0.8941,  0.7839, -0.9566, -0.7040,  0.8349, -0.8429,\n",
      "         0.5260,  1.0000,  0.3988,  0.2915,  0.2070, -0.4509,  0.3234, -0.7112,\n",
      "         0.7073, -0.7636, -0.4493, -0.2528,  0.1974, -0.1834, -0.7970,  0.1371,\n",
      "         0.2759, -0.6157, -0.6439, -0.1309,  0.3869,  0.7567, -0.1551, -0.1575,\n",
      "         0.2578, -0.0711, -0.6940, -0.3948, -0.4347, -1.0000,  0.5399, -1.0000,\n",
      "         0.7128,  0.6155, -0.2521,  0.6373,  0.7132,  0.8144, -0.4154, -0.9492,\n",
      "         0.0490,  0.5232, -0.2458, -0.1341, -0.4549,  0.3408, -0.0545,  0.0695,\n",
      "        -0.8051,  0.7329, -0.2939,  1.0000,  0.2387, -0.7092, -0.8065,  0.3174,\n",
      "        -0.3525,  1.0000, -0.7203, -0.8431,  0.4408, -0.8003, -0.7715,  0.3469,\n",
      "         0.1351, -0.6195, -0.9610,  0.6288,  0.5452, -0.6741,  0.5833, -0.2463,\n",
      "        -0.4161,  0.0590,  0.9378,  0.9481,  0.6915,  0.6850, -0.3048, -0.5970,\n",
      "         0.9140,  0.3908, -0.3312,  0.0704,  1.0000,  0.3300, -0.8420, -0.1792,\n",
      "        -0.8832, -0.2216, -0.7694,  0.2786,  0.2919,  0.7207, -0.1567,  0.8271,\n",
      "        -0.8213, -0.1457, -0.3400, -0.7936,  0.2957, -0.7595, -0.9463, -0.9142,\n",
      "         0.6494, -0.3671,  0.0016,  0.2828, -0.0702,  0.4352,  0.3333, -1.0000,\n",
      "         0.7805,  0.4810,  0.9730,  0.8024,  0.7823,  0.5888,  0.3234, -0.9086,\n",
      "        -0.6842, -0.4138, -0.1805,  0.5940,  0.6941,  0.8276,  0.3598, -0.3997,\n",
      "        -0.6190, -0.9149, -0.9088, -0.9686,  0.4681, -0.7321, -0.6031,  0.9110,\n",
      "         0.3403, -0.2014, -0.5730, -0.9145,  0.4551,  0.4451,  0.0942,  0.0902,\n",
      "         0.3637,  0.5506,  0.8060,  0.9068, -0.9606,  0.5585, -0.7874,  0.5546,\n",
      "         0.9556, -0.8586,  0.2613,  0.5559, -0.4723,  0.2889, -0.3340, -0.7419,\n",
      "         0.7319, -0.3770,  0.5675, -0.3528, -0.0536, -0.3174, -0.2184, -0.5658,\n",
      "        -0.6075,  0.6663,  0.3019,  0.6924,  0.9401, -0.1142, -0.5309, -0.1242,\n",
      "        -0.9343, -0.8099,  0.6097, -0.0124, -0.7966,  0.9148, -0.0402,  0.9651,\n",
      "         0.6443, -0.3715, -0.3640, -0.6907,  0.5956, -0.6568, -0.5451, -0.6698,\n",
      "         0.4106,  0.3329,  1.0000, -0.9138, -0.9451, -0.5781, -0.3327,  0.4728,\n",
      "        -0.6015, -1.0000,  0.2267, -0.6380,  0.9180, -0.6789,  0.9565, -0.5878,\n",
      "        -0.8393, -0.2888,  0.7672,  0.8970, -0.4354, -0.3535,  0.5712,  0.0376,\n",
      "         0.9769,  0.4760, -0.2873, -0.4921,  0.7097, -0.9477, -0.6201,  0.7884]), 'q1_id': 29060, 'q2_embedding': tensor([-8.6166e-01, -3.5545e-01, -8.1525e-01,  7.7919e-01,  6.1370e-01,\n",
      "        -2.5514e-01,  8.1721e-01,  2.2690e-01, -4.1552e-01, -9.9997e-01,\n",
      "        -2.0393e-01,  8.6270e-01,  9.7330e-01,  5.1453e-01,  9.0425e-01,\n",
      "        -7.8751e-01, -2.6530e-01, -6.4865e-01,  2.8228e-01, -2.0520e-01,\n",
      "         6.6982e-01,  9.9999e-01,  1.6390e-01,  3.6438e-01,  3.7573e-01,\n",
      "         8.3776e-01, -7.5062e-01,  9.1445e-01,  9.4363e-01,  7.5589e-01,\n",
      "        -6.1951e-01,  1.8173e-01, -9.8718e-01, -1.6606e-01, -8.6838e-01,\n",
      "        -9.8947e-01,  2.8077e-01, -6.1940e-01, -2.0391e-02,  2.9509e-02,\n",
      "        -8.6068e-01,  2.1721e-01,  9.9994e-01, -5.4927e-01,  4.8309e-01,\n",
      "        -4.0050e-01, -1.0000e+00,  2.9551e-01, -8.3929e-01,  6.8597e-01,\n",
      "         7.8214e-01,  5.6069e-01,  2.5626e-01,  4.3493e-01,  5.3414e-01,\n",
      "        -3.2916e-01, -2.5913e-01,  2.1233e-01, -2.4010e-01, -5.0877e-01,\n",
      "        -6.6341e-01,  4.8959e-01, -7.3844e-01, -8.3534e-01,  7.7227e-01,\n",
      "         7.4067e-01, -1.9963e-01, -3.1633e-01, -1.5393e-01,  8.1981e-03,\n",
      "         8.2760e-01,  1.5796e-01, -1.5767e-01, -8.5639e-01,  2.0450e-01,\n",
      "         2.8960e-01, -6.9958e-01,  1.0000e+00, -5.1784e-01, -9.6565e-01,\n",
      "         7.9289e-01,  6.5052e-01,  6.5868e-01, -1.4155e-01,  5.2800e-01,\n",
      "        -1.0000e+00,  4.9148e-01, -9.7655e-02, -9.8622e-01,  3.0146e-01,\n",
      "         4.8468e-01, -1.7833e-01,  8.0202e-01,  6.1272e-01, -6.9867e-01,\n",
      "        -4.1048e-01, -2.0564e-01, -7.6802e-01, -2.6877e-01, -5.2216e-01,\n",
      "         5.2102e-02, -2.8969e-01, -3.4423e-01, -3.1963e-01,  3.5258e-01,\n",
      "        -4.7227e-01, -4.0581e-01,  6.3393e-01,  2.1468e-01,  6.2765e-01,\n",
      "         5.6798e-01, -2.9533e-01,  4.4872e-01, -9.2171e-01,  6.0923e-01,\n",
      "        -3.7312e-01, -9.8484e-01, -6.3244e-01, -9.8415e-01,  5.6194e-01,\n",
      "        -3.7591e-01, -2.8423e-01,  9.0228e-01, -3.5792e-01,  4.7245e-01,\n",
      "        -2.5851e-01, -6.1549e-01, -1.0000e+00, -4.9192e-01, -7.8323e-01,\n",
      "        -3.0451e-01, -3.5224e-01, -9.6708e-01, -9.6049e-01,  6.2692e-01,\n",
      "         9.5319e-01,  2.6023e-01,  9.9971e-01, -3.3975e-01,  8.9671e-01,\n",
      "         1.7068e-01, -6.3850e-01,  2.4467e-01, -4.6389e-01,  8.6192e-01,\n",
      "         2.2420e-01, -5.0704e-01,  2.0545e-01, -4.7743e-01, -1.4598e-01,\n",
      "        -7.8000e-01, -1.9565e-01, -7.0882e-01, -8.9983e-01, -3.6702e-01,\n",
      "         9.1886e-01, -3.3540e-01, -8.1101e-01,  4.3915e-02, -2.1869e-01,\n",
      "        -3.6443e-01,  7.9094e-01,  7.7226e-01,  4.9055e-01, -4.2521e-01,\n",
      "         4.7915e-01,  3.3720e-01,  5.2622e-01, -8.6252e-01,  2.1800e-01,\n",
      "         3.7323e-01, -4.0946e-01, -8.5162e-01, -9.6378e-01, -3.7479e-01,\n",
      "         3.5514e-01,  9.8611e-01,  7.7808e-01,  2.5086e-01,  3.7961e-01,\n",
      "        -3.8856e-01,  2.8507e-01, -9.4205e-01,  9.6997e-01, -4.8175e-02,\n",
      "         2.3238e-01, -5.2729e-01,  4.0721e-01, -8.2052e-01,  2.6039e-01,\n",
      "         7.5859e-01, -1.5475e-01, -7.8557e-01, -1.9331e-01, -5.6873e-01,\n",
      "        -4.9297e-01, -7.7845e-01,  4.1206e-01, -2.1669e-01, -3.9531e-01,\n",
      "        -5.1171e-02,  9.0736e-01,  9.4314e-01,  6.5031e-01,  1.2322e-02,\n",
      "         6.7609e-01, -7.6950e-01, -3.4896e-01,  1.0620e-01,  2.6898e-01,\n",
      "        -5.7082e-02,  9.8472e-01, -8.4020e-01,  2.8052e-02, -8.8870e-01,\n",
      "        -9.6542e-01,  6.1480e-02, -8.5693e-01, -1.5182e-01, -6.8562e-01,\n",
      "         6.0917e-01, -1.0269e-01,  8.9617e-02,  3.9621e-01, -8.8038e-01,\n",
      "        -7.9712e-01,  3.8058e-01, -4.1870e-01,  4.3581e-01, -2.6459e-01,\n",
      "         8.9893e-01,  9.1129e-01, -5.8313e-01,  4.7436e-01,  9.4699e-01,\n",
      "        -8.5348e-01, -8.0621e-01,  6.8213e-01, -2.5092e-01,  8.5957e-01,\n",
      "        -5.7678e-01,  9.8186e-01,  8.2649e-01,  6.4004e-01, -8.0346e-01,\n",
      "        -7.1701e-01, -7.7135e-01, -5.3198e-01,  1.5274e-02,  1.0262e-01,\n",
      "         7.4046e-01,  7.1519e-01,  3.6972e-01,  1.6141e-01, -5.2736e-01,\n",
      "         9.8603e-01, -8.9539e-01, -9.5317e-01, -4.5815e-01,  4.0653e-02,\n",
      "        -9.8633e-01,  8.4930e-01,  3.3050e-01,  5.7344e-01, -4.1259e-01,\n",
      "        -7.5679e-01, -9.5271e-01,  6.6246e-01,  1.3826e-01,  9.7352e-01,\n",
      "        -5.1215e-01, -8.8555e-01, -5.0221e-01, -9.0837e-01, -1.1045e-01,\n",
      "        -2.3964e-01,  1.3198e-02, -6.3505e-02, -9.3320e-01,  6.2782e-01,\n",
      "         5.7680e-01,  5.3178e-01, -7.6252e-01,  9.9641e-01,  1.0000e+00,\n",
      "         9.6317e-01,  8.8213e-01,  7.3621e-01, -9.9983e-01, -6.2981e-01,\n",
      "         9.9999e-01, -9.6605e-01, -1.0000e+00, -9.1191e-01, -5.5414e-01,\n",
      "         4.0544e-01, -1.0000e+00, -3.3108e-01, -3.7512e-02, -8.2331e-01,\n",
      "         6.2336e-01,  9.5362e-01,  9.7285e-01, -1.0000e+00,  6.7940e-01,\n",
      "         8.6773e-01, -7.3772e-01,  8.2994e-01, -4.8026e-01,  9.4972e-01,\n",
      "         7.0283e-01,  6.5278e-01, -3.3338e-01,  4.3581e-01, -9.2719e-01,\n",
      "        -8.5593e-01, -4.7075e-01, -7.3561e-01,  9.9749e-01,  1.9163e-01,\n",
      "        -8.0396e-01, -7.9802e-01,  5.2375e-01, -1.4679e-01, -3.0956e-01,\n",
      "        -9.4894e-01, -2.6783e-01,  9.6481e-02,  7.3200e-01,  2.6402e-01,\n",
      "         3.7366e-01, -5.2944e-01,  3.0961e-01,  3.4489e-01,  1.0361e-01,\n",
      "         6.9018e-01, -9.3833e-01, -3.5891e-01,  3.6237e-02, -3.3028e-01,\n",
      "        -5.3452e-01, -9.6027e-01,  9.1584e-01, -2.5884e-01,  5.1066e-01,\n",
      "         1.0000e+00,  5.1840e-01, -7.7550e-01,  6.8740e-01,  1.6397e-01,\n",
      "        -8.1278e-01,  1.0000e+00,  8.2863e-01, -9.6725e-01, -6.3770e-01,\n",
      "         5.7359e-01, -5.1309e-01, -5.6592e-01,  9.9841e-01, -1.7165e-01,\n",
      "        -5.3547e-01, -3.4643e-01,  9.7559e-01, -9.8736e-01,  9.9481e-01,\n",
      "        -8.5199e-01, -9.5508e-01,  9.4533e-01,  9.0113e-01, -5.6834e-01,\n",
      "        -6.7933e-01,  1.7108e-01, -7.3395e-01,  4.2970e-01, -9.0266e-01,\n",
      "         5.9558e-01,  4.1061e-01, -1.8400e-01,  8.8633e-01, -6.7539e-01,\n",
      "        -6.2283e-01,  2.6724e-01, -7.0185e-01,  1.0681e-01,  8.8638e-01,\n",
      "         4.7452e-01, -3.6876e-01, -1.0902e-02, -3.5563e-01, -6.5704e-01,\n",
      "        -9.6538e-01,  5.3982e-01,  1.0000e+00, -1.0882e-01,  7.5131e-01,\n",
      "        -3.3517e-01, -1.5580e-01, -4.7502e-02,  5.8163e-01,  6.1361e-01,\n",
      "        -2.8170e-01, -7.7940e-01,  5.5515e-01, -8.6577e-01, -9.8412e-01,\n",
      "         5.9782e-01,  1.9255e-01, -3.2005e-01,  9.9998e-01,  4.5043e-01,\n",
      "         2.3404e-01,  1.0247e-01,  9.0688e-01,  1.7308e-01,  3.2256e-01,\n",
      "         6.1539e-01,  9.8359e-01, -2.9549e-01,  6.6647e-01,  7.1954e-01,\n",
      "        -7.0216e-01, -3.3901e-01, -6.6650e-01, -6.9731e-02, -9.5153e-01,\n",
      "         5.8762e-02, -9.1018e-01,  9.6654e-01,  7.0392e-01,  4.1880e-01,\n",
      "         3.0159e-01,  6.4619e-01,  1.0000e+00, -8.2387e-01,  5.5607e-01,\n",
      "         2.6926e-01,  6.3420e-01, -9.9975e-01, -6.8732e-01, -3.2786e-01,\n",
      "        -1.5433e-01, -6.2796e-01, -3.3270e-01,  3.5901e-01, -9.4525e-01,\n",
      "         6.1868e-01,  3.9418e-01, -9.6091e-01, -9.7515e-01,  9.9492e-02,\n",
      "         7.8261e-01, -2.5103e-04, -9.8119e-01, -5.4404e-01, -6.0214e-01,\n",
      "         3.6219e-01, -4.0226e-01, -8.9060e-01,  2.2643e-01, -3.3812e-01,\n",
      "         3.4856e-01, -2.7152e-01,  6.5563e-01,  7.6363e-01,  6.6880e-01,\n",
      "        -7.5512e-01, -2.6547e-01, -2.5785e-01, -7.7000e-01,  8.3665e-01,\n",
      "        -8.1959e-01, -7.1425e-01, -1.6678e-01,  1.0000e+00, -7.2490e-01,\n",
      "         9.0562e-01,  6.8574e-01,  5.6207e-01, -2.7677e-01,  3.5044e-01,\n",
      "         9.2893e-01,  4.1006e-01, -7.0106e-01, -6.5209e-01, -1.5681e-02,\n",
      "        -4.3261e-01,  7.4111e-01,  6.7242e-01,  7.7201e-01,  7.6362e-01,\n",
      "         7.9919e-01,  3.3345e-02, -7.5918e-02,  1.0879e-01,  9.9604e-01,\n",
      "        -1.0003e-01,  2.3574e-02, -4.8750e-01, -1.8083e-01, -4.4782e-01,\n",
      "         3.5476e-02,  1.0000e+00,  2.4575e-01,  4.2187e-01, -9.8310e-01,\n",
      "        -6.2413e-01, -8.5081e-01,  1.0000e+00,  8.0709e-01, -6.0093e-01,\n",
      "         7.1014e-01,  4.0939e-01, -1.6140e-01,  5.1290e-01, -2.3710e-01,\n",
      "        -3.1410e-01,  2.3329e-01,  1.0703e-01,  9.1187e-01, -5.2016e-01,\n",
      "        -9.6171e-01, -6.9553e-01,  3.4194e-01, -9.4135e-01,  9.9992e-01,\n",
      "        -5.9941e-01, -3.8077e-01, -4.8066e-01,  1.7143e-01, -7.2957e-02,\n",
      "        -8.6932e-02, -9.5886e-01, -1.4069e-01,  1.6878e-01,  9.1996e-01,\n",
      "         2.3143e-01, -6.8534e-01, -8.7906e-01,  5.3366e-01,  6.5765e-01,\n",
      "        -7.5796e-01, -8.8295e-01,  9.4159e-01, -9.5733e-01,  3.9005e-01,\n",
      "         1.0000e+00,  4.6408e-01, -1.8343e-01,  2.4042e-01, -4.5817e-01,\n",
      "         2.3040e-01, -3.8932e-01,  7.6399e-01, -9.2572e-01, -4.2324e-01,\n",
      "        -2.4254e-01,  2.5046e-01, -1.1580e-01, -3.4900e-01,  4.9889e-01,\n",
      "         1.6727e-01, -5.9786e-01, -5.8863e-01, -2.2637e-01,  4.4960e-01,\n",
      "         8.1668e-01, -2.2625e-01, -1.1460e-01,  2.1571e-01, -8.4931e-02,\n",
      "        -8.1890e-01, -3.1946e-01, -3.8830e-01, -9.9999e-01,  5.8395e-01,\n",
      "        -1.0000e+00,  4.9954e-01,  4.0143e-03, -3.1610e-01,  8.5417e-01,\n",
      "         5.2885e-01,  6.8677e-01, -6.5357e-01, -6.8431e-01,  5.7873e-01,\n",
      "         6.1064e-01, -1.8796e-01,  1.7868e-01, -6.6039e-01,  2.5805e-01,\n",
      "        -3.1360e-02,  2.5663e-02, -4.3999e-01,  7.8801e-01, -2.8687e-01,\n",
      "         1.0000e+00,  1.9653e-01, -6.6028e-01, -9.0982e-01,  2.3197e-01,\n",
      "        -3.8097e-01,  1.0000e+00, -8.7742e-01, -9.4506e-01,  3.7212e-01,\n",
      "        -7.5413e-01, -8.3542e-01,  3.8259e-01,  1.2187e-01, -5.5042e-01,\n",
      "        -8.0464e-01,  9.0563e-01,  8.1439e-01, -6.4820e-01,  5.7218e-01,\n",
      "        -3.4701e-01, -4.5446e-01,  4.8975e-02,  6.7842e-01,  9.8086e-01,\n",
      "         7.6573e-01,  8.1503e-01,  3.8398e-01, -3.9296e-01,  9.6369e-01,\n",
      "         3.8377e-01,  2.4540e-01,  6.7583e-02,  1.0000e+00,  3.8428e-01,\n",
      "        -9.2515e-01,  9.3645e-02, -9.7188e-01, -2.1828e-01, -9.3049e-01,\n",
      "         2.4616e-01,  3.0426e-01,  8.3656e-01, -1.8455e-01,  9.2917e-01,\n",
      "        -3.5780e-01, -1.2782e-01, -1.8584e-01, -1.5638e-01,  4.3916e-01,\n",
      "        -8.5787e-01, -9.7831e-01, -9.7648e-01,  5.3806e-01, -4.1969e-01,\n",
      "        -9.7791e-02,  3.0994e-01,  1.1138e-01,  4.4268e-01,  3.1196e-01,\n",
      "        -1.0000e+00,  8.7648e-01,  4.7030e-01,  8.4328e-01,  9.3418e-01,\n",
      "         5.8135e-01,  5.4543e-01,  3.5866e-01, -9.7818e-01, -9.1238e-01,\n",
      "        -3.7814e-01, -2.5070e-01,  7.4427e-01,  7.1484e-01,  8.9235e-01,\n",
      "         3.8457e-01, -5.2999e-01, -4.8498e-01, -5.8354e-01, -5.2790e-01,\n",
      "        -9.9012e-01,  5.0168e-01, -2.9051e-01, -8.1976e-01,  9.4941e-01,\n",
      "        -1.6318e-01, -2.5596e-01,  1.8254e-02, -7.3626e-01,  7.4723e-01,\n",
      "         6.5806e-01,  2.2336e-01,  3.1092e-02,  5.4758e-01,  8.0006e-01,\n",
      "         9.0526e-01,  9.7625e-01, -7.4909e-01,  7.8951e-01, -3.5874e-01,\n",
      "         5.6128e-01,  9.0363e-01, -9.4446e-01,  1.6413e-01,  4.1025e-01,\n",
      "        -5.3351e-01,  2.7979e-01, -3.4221e-01, -9.1114e-01,  8.6037e-01,\n",
      "        -2.8108e-01,  5.7373e-01, -3.3604e-01, -3.0518e-02, -4.1510e-01,\n",
      "        -2.2436e-01, -7.5914e-01, -5.2162e-01,  6.8035e-01,  2.6465e-01,\n",
      "         8.6482e-01,  7.6967e-01, -7.8986e-02, -5.0536e-01, -1.2727e-01,\n",
      "        -6.9079e-01, -8.8110e-01,  8.1430e-01,  1.0151e-02, -3.5820e-01,\n",
      "         8.1105e-01, -1.3058e-01,  9.1592e-01,  1.8638e-01, -4.0011e-01,\n",
      "        -3.6788e-01, -7.6691e-01,  7.1200e-01, -6.5735e-01, -4.8955e-01,\n",
      "        -6.0672e-01,  4.3722e-01,  3.6387e-01,  9.9999e-01, -6.6598e-01,\n",
      "        -6.4862e-01, -4.8198e-01, -2.4116e-01,  4.7205e-01, -5.4329e-01,\n",
      "        -1.0000e+00,  3.8435e-01, -3.0017e-01,  6.8243e-01, -2.8776e-01,\n",
      "         7.9974e-01, -4.8917e-01, -9.3900e-01, -1.6678e-01,  5.4150e-01,\n",
      "         7.1187e-01, -4.5019e-01, -2.5864e-01,  6.1258e-01,  5.6273e-01,\n",
      "         8.4875e-01,  7.5869e-01, -4.5974e-01,  1.5778e-01,  6.9641e-01,\n",
      "        -8.7400e-01, -6.9358e-01,  8.6327e-01]), 'q2_id': 9004, 'label': 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     acc \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_samples\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m acc, mrr\n\u001b[0;32m---> 43\u001b[0m tacc, tmrr \u001b[39m=\u001b[39m special_performance(X_train)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSpecial | Training Accuracy\u001b[39m\u001b[39m\"\u001b[39m, tacc, \u001b[39m\"\u001b[39m\u001b[39m | Training MRR:\u001b[39m\u001b[39m\"\u001b[39m, tmrr)\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mspecial_performance\u001b[0;34m(data_partition, k)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# we also need to get the real id \u001b[39;00m\n\u001b[1;32m     22\u001b[0m original_label \u001b[39m=\u001b[39m entry[\u001b[39m\"\u001b[39m\u001b[39mq2_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m nearest_neighbors \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mkneighbors(cur_embed\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), k, return_distance\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[39m# we need to start at the second index, because the first index is always gonan be itself \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m# acc\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39munique_embeddings_id[nearest_neighbors[\u001b[39m1\u001b[39m]] \u001b[39m==\u001b[39m original_label: \n",
      "\u001b[0;31mNameError\u001b[0m: name 'knn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from collections import defaultdict\n",
    "\n",
    "from model import SpecialDataLoader\n",
    "\n",
    "special_dataloader = SpecialDataLoader()\n",
    "special_X_train, special_X_val, special_X_test = get_training_splits(special_dataloader)\n",
    "\n",
    "def special_performance(data_partition, k = 10):\n",
    "    \n",
    "    # knn = NearestNeighbors(n_neighbors=10, metric=\"cosine\", n_jobs=-1, leaf_size=300)\n",
    "    # knn.fit(cur_samples)\n",
    "    mrr = 0\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    # counts = defaultdict(int)\n",
    "    for entry in data_partition:\n",
    "        # print(entry)\n",
    "        # counts[]\n",
    "        # if the label is 0, we don't need to check \n",
    "        if entry[\"label\"] == -1:\n",
    "            continue\n",
    "        num_samples += 1\n",
    "        # first get the new embedding using our model\n",
    "        # We use Q2 because Q2 is the duplicate \n",
    "        cur_embed = model.ff(entry[\"q1_embedding\"])\n",
    "        # we also need to get the real id \n",
    "        original_label = entry[\"q2_id\"]\n",
    "        \n",
    "        nearest_neighbors = knn.kneighbors(cur_embed.detach().reshape(1, -1), k, return_distance=False).tolist()[0]\n",
    "        \n",
    "        # we need to start at the second index, because the first index is always gonan be itself \n",
    "        \n",
    "        # acc\n",
    "        if data.unique_embeddings_id[nearest_neighbors[1]] == original_label: \n",
    "            acc += 1\n",
    "            \n",
    "        # mrr\n",
    "        for i, cur_neighbor in enumerate(nearest_neighbors[1:]):\n",
    "            \n",
    "            if data.unique_embeddings_id[cur_neighbor] == original_label:\n",
    "                mrr += 1 / (1 + i)\n",
    "                break\n",
    "    \n",
    "    mrr /= num_samples\n",
    "    acc /= num_samples\n",
    "    return acc, mrr\n",
    "\n",
    "special_tacc, special_tmrr = special_performance(special_X_train)\n",
    "print(\"Special | Training Accuracy\", special_tacc, \" | Training MRR:\", special_tmrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully\n",
      "Epoch: 1170  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00775709219858156\n",
      "Epoch: 1170  | Validation Accuracy 0.0  | Validation MRR: 0.001287001287001287\n",
      "Epoch: 1170 with a loss of: 0.000474606734351255\n",
      "\n",
      "\n",
      "Epoch: 1171  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005984042553191489\n",
      "Epoch: 1171  | Validation Accuracy 0.0  | Validation MRR: 0.0022522522522522522\n",
      "Epoch: 1171 with a loss of: 2.499110996723175e-05\n",
      "\n",
      "\n",
      "Epoch: 1172  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007978723404255319\n",
      "Epoch: 1172  | Validation Accuracy 0.0  | Validation MRR: 0.001287001287001287\n",
      "Epoch: 1172 with a loss of: 0.00018470136390533298\n",
      "\n",
      "\n",
      "Epoch: 1173  | Training Accuracy 0.010638297872340425  | Training MRR: 0.013076241134751771\n",
      "Epoch: 1173  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1173 with a loss of: 0.000900263786315918\n",
      "\n",
      "\n",
      "Epoch: 1174  | Training Accuracy 0.0  | Training MRR: 0.0007598784194528875\n",
      "Epoch: 1174  | Validation Accuracy 0.0  | Validation MRR: 0.001287001287001287\n",
      "Epoch: 1174 with a loss of: 1.882001757621765e-05\n",
      "\n",
      "\n",
      "Epoch: 1175  | Training Accuracy 0.010638297872340425  | Training MRR: 0.011398176291793313\n",
      "Epoch: 1175  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.012012012012012012\n",
      "Epoch: 1175 with a loss of: 7.3838979005813595e-06\n",
      "\n",
      "\n",
      "Epoch: 1176  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006648936170212766\n",
      "Epoch: 1176  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1176 with a loss of: 7.18921422958374e-06\n",
      "\n",
      "\n",
      "Epoch: 1177  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0060790273556231\n",
      "Epoch: 1177  | Validation Accuracy 0.0  | Validation MRR: 0.001001001001001001\n",
      "Epoch: 1177 with a loss of: 1.1160001158714294e-05\n",
      "\n",
      "\n",
      "Epoch: 1178  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005984042553191489\n",
      "Epoch: 1178  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1178 with a loss of: 3.6921054124832154e-05\n",
      "\n",
      "\n",
      "Epoch: 1179  | Training Accuracy 0.010638297872340425  | Training MRR: 0.011894208037825059\n",
      "Epoch: 1179  | Validation Accuracy 0.0  | Validation MRR: 0.0015015015015015015\n",
      "Epoch: 1179 with a loss of: 2.566799521446228e-06\n",
      "\n",
      "\n",
      "Epoch: 1180  | Training Accuracy 0.010638297872340425  | Training MRR: 0.013297872340425532\n",
      "Epoch: 1180  | Validation Accuracy 0.0  | Validation MRR: 0.005341055341055341\n",
      "Epoch: 1180 with a loss of: 0.00026224380591884257\n",
      "\n",
      "\n",
      "Epoch: 1181  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006870567375886525\n",
      "Epoch: 1181  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1181 with a loss of: 5.377903580665588e-06\n",
      "\n",
      "\n",
      "Epoch: 1182  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007852077001013171\n",
      "Epoch: 1182  | Validation Accuracy 0.0  | Validation MRR: 0.0045045045045045045\n",
      "Epoch: 1182 with a loss of: 1.1615976691246032e-05\n",
      "\n",
      "\n",
      "Epoch: 1183  | Training Accuracy 0.005319148936170213  | Training MRR: 0.011968085106382979\n",
      "Epoch: 1183  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1183 with a loss of: 0.0006621294096112252\n",
      "\n",
      "\n",
      "Epoch: 1184  | Training Accuracy 0.0  | Training MRR: 0.0042109929078014184\n",
      "Epoch: 1184  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1184 with a loss of: 5.463726818561554e-05\n",
      "\n",
      "\n",
      "Epoch: 1185  | Training Accuracy 0.005319148936170213  | Training MRR: 0.009707446808510638\n",
      "Epoch: 1185  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1185 with a loss of: 0.005833755005151034\n",
      "\n",
      "\n",
      "Epoch: 1186  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00591016548463357\n",
      "Epoch: 1186  | Validation Accuracy 0.0  | Validation MRR: 0.003003003003003003\n",
      "Epoch: 1186 with a loss of: 0.002270765332505107\n",
      "\n",
      "\n",
      "Epoch: 1187  | Training Accuracy 0.0  | Training MRR: 0.0010638297872340426\n",
      "Epoch: 1187  | Validation Accuracy 0.0  | Validation MRR: 0.001001001001001001\n",
      "Epoch: 1187 with a loss of: 1.1959373950958253e-05\n",
      "\n",
      "\n",
      "Epoch: 1188  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00816869300911854\n",
      "Epoch: 1188  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1188 with a loss of: 2.501673996448517e-05\n",
      "\n",
      "\n",
      "Epoch: 1189  | Training Accuracy 0.005319148936170213  | Training MRR: 0.01006838905775076\n",
      "Epoch: 1189  | Validation Accuracy 0.0  | Validation MRR: 0.002927927927927928\n",
      "Epoch: 1189 with a loss of: 1.2690126895904541e-05\n",
      "\n",
      "\n",
      "Epoch: 1190  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007630445795339413\n",
      "Epoch: 1190  | Validation Accuracy 0.0  | Validation MRR: 0.003928928928928929\n",
      "Epoch: 1190 with a loss of: 2.127431333065033e-05\n",
      "\n",
      "\n",
      "Epoch: 1191  | Training Accuracy 0.010638297872340425  | Training MRR: 0.012854609929078015\n",
      "Epoch: 1191  | Validation Accuracy 0.0  | Validation MRR: 0.005341055341055341\n",
      "Epoch: 1191 with a loss of: 1.266859471797943e-05\n",
      "\n",
      "\n",
      "Epoch: 1192  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007313829787234043\n",
      "Epoch: 1192  | Validation Accuracy 0.0  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1192 with a loss of: 1.7585605382919313e-05\n",
      "\n",
      "\n",
      "Epoch: 1193  | Training Accuracy 0.0  | Training MRR: 0.0006648936170212766\n",
      "Epoch: 1193  | Validation Accuracy 0.0  | Validation MRR: 0.0018018018018018018\n",
      "Epoch: 1193 with a loss of: 1.6338974237442017e-05\n",
      "\n",
      "\n",
      "Epoch: 1194  | Training Accuracy 0.005319148936170213  | Training MRR: 0.01152482269503546\n",
      "Epoch: 1194  | Validation Accuracy 0.0  | Validation MRR: 0.0015015015015015015\n",
      "Epoch: 1194 with a loss of: 6.055459380149841e-06\n",
      "\n",
      "\n",
      "Epoch: 1195  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0062056737588652485\n",
      "Epoch: 1195  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1195 with a loss of: 7.186830043792725e-06\n",
      "\n",
      "\n",
      "Epoch: 1196  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006382978723404255\n",
      "Epoch: 1196  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1196 with a loss of: 2.8689205646514893e-06\n",
      "\n",
      "\n",
      "Epoch: 1197  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1197  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1197 with a loss of: 3.1845271587371827e-06\n",
      "\n",
      "\n",
      "Epoch: 1198  | Training Accuracy 0.0  | Training MRR: 0.0031978216818642348\n",
      "Epoch: 1198  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1198 with a loss of: 6.09666109085083e-06\n",
      "\n",
      "\n",
      "Epoch: 1199  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007852077001013171\n",
      "Epoch: 1199  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1199 with a loss of: 1.5135109424591063e-06\n",
      "\n",
      "\n",
      "Epoch: 1200  | Training Accuracy 0.0  | Training MRR: 0.0010638297872340426\n",
      "Epoch: 1200  | Validation Accuracy 0.0  | Validation MRR: 0.006006006006006006\n",
      "Epoch: 1200 with a loss of: 9.077265858650207e-06\n",
      "\n",
      "\n",
      "Epoch: 1201  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00591016548463357\n",
      "Epoch: 1201  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1201 with a loss of: 3.7580728530883787e-06\n",
      "\n",
      "\n",
      "Epoch: 1202  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1202  | Validation Accuracy 0.0  | Validation MRR: 0.0033033033033033035\n",
      "Epoch: 1202 with a loss of: 4.442781209945678e-06\n",
      "\n",
      "\n",
      "Epoch: 1203  | Training Accuracy 0.010638297872340425  | Training MRR: 0.01170212765957447\n",
      "Epoch: 1203  | Validation Accuracy 0.0  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1203 with a loss of: 3.7334859371185304e-06\n",
      "\n",
      "\n",
      "Epoch: 1204  | Training Accuracy 0.0  | Training MRR: 0.0012559101654846334\n",
      "Epoch: 1204  | Validation Accuracy 0.0  | Validation MRR: 0.0022880022880022877\n",
      "Epoch: 1204 with a loss of: 1.8244236707687378e-06\n",
      "\n",
      "\n",
      "Epoch: 1205  | Training Accuracy 0.005319148936170213  | Training MRR: 0.010385005065856129\n",
      "Epoch: 1205  | Validation Accuracy 0.0  | Validation MRR: 0.003003003003003003\n",
      "Epoch: 1205 with a loss of: 0.003512293845415115\n",
      "\n",
      "\n",
      "Epoch: 1206  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1206  | Validation Accuracy 0.0  | Validation MRR: 0.001001001001001001\n",
      "Epoch: 1206 with a loss of: 0.0003384553641080856\n",
      "\n",
      "\n",
      "Epoch: 1207  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00591016548463357\n",
      "Epoch: 1207  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1207 with a loss of: 0.0016871117055416106\n",
      "\n",
      "\n",
      "Epoch: 1208  | Training Accuracy 0.005319148936170213  | Training MRR: 0.010593971631205673\n",
      "Epoch: 1208  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1208 with a loss of: 1.0886937379837036e-05\n",
      "\n",
      "\n",
      "Epoch: 1209  | Training Accuracy 0.0  | Training MRR: 0.004432624113475177\n",
      "Epoch: 1209  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1209 with a loss of: 1.1445879936218262e-05\n",
      "\n",
      "\n",
      "Epoch: 1210  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007334937521107734\n",
      "Epoch: 1210  | Validation Accuracy 0.0  | Validation MRR: 0.0025025025025025025\n",
      "Epoch: 1210 with a loss of: 8.238926529884338e-06\n",
      "\n",
      "\n",
      "Epoch: 1211  | Training Accuracy 0.005319148936170213  | Training MRR: 0.010638297872340425\n",
      "Epoch: 1211  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.014264264264264264\n",
      "Epoch: 1211 with a loss of: 7.0855021476745605e-06\n",
      "\n",
      "\n",
      "Epoch: 1212  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0067966903073286055\n",
      "Epoch: 1212  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1212 with a loss of: 3.3540278673172e-06\n",
      "\n",
      "\n",
      "Epoch: 1213  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1213  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1213 with a loss of: 1.270100474357605e-05\n",
      "\n",
      "\n",
      "Epoch: 1214  | Training Accuracy 0.005319148936170213  | Training MRR: 0.009219858156028368\n",
      "Epoch: 1214  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.012012012012012012\n",
      "Epoch: 1214 with a loss of: 3.4570693969726562e-06\n",
      "\n",
      "\n",
      "Epoch: 1215  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00948581560283688\n",
      "Epoch: 1215  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1215 with a loss of: 0.0025337836518883705\n",
      "\n",
      "\n",
      "Epoch: 1216  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1216  | Validation Accuracy 0.0  | Validation MRR: 0.0015015015015015015\n",
      "Epoch: 1216 with a loss of: 0.001964528262615204\n",
      "\n",
      "\n",
      "Epoch: 1217  | Training Accuracy 0.005319148936170213  | Training MRR: 0.009013002364066195\n",
      "Epoch: 1217  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1217 with a loss of: 0.0018736403435468674\n",
      "\n",
      "\n",
      "Epoch: 1218  | Training Accuracy 0.005319148936170213  | Training MRR: 0.008738601823708206\n",
      "Epoch: 1218  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1218 with a loss of: 0.0017056280374526977\n",
      "\n",
      "\n",
      "Epoch: 1219  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007313829787234043\n",
      "Epoch: 1219  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1219 with a loss of: 0.0016571246087551117\n",
      "\n",
      "\n",
      "Epoch: 1220  | Training Accuracy 0.015957446808510637  | Training MRR: 0.018617021276595744\n",
      "Epoch: 1220  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1220 with a loss of: 0.0003686455637216568\n",
      "\n",
      "\n",
      "Epoch: 1221  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007978723404255319\n",
      "Epoch: 1221  | Validation Accuracy 0.0  | Validation MRR: 0.0015015015015015015\n",
      "Epoch: 1221 with a loss of: 1.9670724868774413e-05\n",
      "\n",
      "\n",
      "Epoch: 1222  | Training Accuracy 0.0  | Training MRR: 0.0038394967916244504\n",
      "Epoch: 1222  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1222 with a loss of: 2.327457070350647e-05\n",
      "\n",
      "\n",
      "Epoch: 1223  | Training Accuracy 0.0  | Training MRR: 0.0\n",
      "Epoch: 1223  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1223 with a loss of: 0.000119805708527565\n",
      "\n",
      "\n",
      "Epoch: 1224  | Training Accuracy 0.015957446808510637  | Training MRR: 0.015957446808510637\n",
      "Epoch: 1224  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1224 with a loss of: 0.002115224190056324\n",
      "\n",
      "\n",
      "Epoch: 1225  | Training Accuracy 0.010638297872340425  | Training MRR: 0.015957446808510637\n",
      "Epoch: 1225  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1225 with a loss of: 1.6477480530738832e-05\n",
      "\n",
      "\n",
      "Epoch: 1226  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007166075650118204\n",
      "Epoch: 1226  | Validation Accuracy 0.0  | Validation MRR: 0.0022522522522522522\n",
      "Epoch: 1226 with a loss of: 6.73789530992508e-05\n",
      "\n",
      "\n",
      "Epoch: 1227  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0062056737588652485\n",
      "Epoch: 1227  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1227 with a loss of: 6.712377071380615e-06\n",
      "\n",
      "\n",
      "Epoch: 1228  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006648936170212766\n",
      "Epoch: 1228  | Validation Accuracy 0.0  | Validation MRR: 0.00563063063063063\n",
      "Epoch: 1228 with a loss of: 1.4313831925392151e-05\n",
      "\n",
      "\n",
      "Epoch: 1229  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007852077001013171\n",
      "Epoch: 1229  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1229 with a loss of: 1.3776943087577819e-05\n",
      "\n",
      "\n",
      "Epoch: 1230  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00591016548463357\n",
      "Epoch: 1230  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1230 with a loss of: 7.972121238708496e-06\n",
      "\n",
      "\n",
      "Epoch: 1231  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0060790273556231\n",
      "Epoch: 1231  | Validation Accuracy 0.0  | Validation MRR: 0.003003003003003003\n",
      "Epoch: 1231 with a loss of: 5.994513630867005e-06\n",
      "\n",
      "\n",
      "Epoch: 1232  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0060790273556231\n",
      "Epoch: 1232  | Validation Accuracy 0.0  | Validation MRR: 0.0018018018018018018\n",
      "Epoch: 1232 with a loss of: 7.053688168525696e-06\n",
      "\n",
      "\n",
      "Epoch: 1233  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1233  | Validation Accuracy 0.0  | Validation MRR: 0.0018018018018018018\n",
      "Epoch: 1233 with a loss of: 2.4874508380889894e-06\n",
      "\n",
      "\n",
      "Epoch: 1234  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0062056737588652485\n",
      "Epoch: 1234  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1234 with a loss of: 2.466067671775818e-06\n",
      "\n",
      "\n",
      "Epoch: 1235  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0076832151300236405\n",
      "Epoch: 1235  | Validation Accuracy 0.0  | Validation MRR: 0.0018018018018018018\n",
      "Epoch: 1235 with a loss of: 2.424195408821106e-06\n",
      "\n",
      "\n",
      "Epoch: 1236  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007269503546099291\n",
      "Epoch: 1236  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1236 with a loss of: 8.368939161300659e-06\n",
      "\n",
      "\n",
      "Epoch: 1237  | Training Accuracy 0.010638297872340425  | Training MRR: 0.013002364066193851\n",
      "Epoch: 1237  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1237 with a loss of: 0.00216219212859869\n",
      "\n",
      "\n",
      "Epoch: 1238  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007239952718676123\n",
      "Epoch: 1238  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.012387387387387387\n",
      "Epoch: 1238 with a loss of: 1.172684133052826e-05\n",
      "\n",
      "\n",
      "Epoch: 1239  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0067966903073286055\n",
      "Epoch: 1239  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1239 with a loss of: 5.372911691665649e-06\n",
      "\n",
      "\n",
      "Epoch: 1240  | Training Accuracy 0.005319148936170213  | Training MRR: 0.008569739952718676\n",
      "Epoch: 1240  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1240 with a loss of: 6.624892354011536e-05\n",
      "\n",
      "\n",
      "Epoch: 1241  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007712765957446808\n",
      "Epoch: 1241  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1241 with a loss of: 1.0115951299667358e-05\n",
      "\n",
      "\n",
      "Epoch: 1242  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00975177304964539\n",
      "Epoch: 1242  | Validation Accuracy 0.0  | Validation MRR: 0.00563063063063063\n",
      "Epoch: 1242 with a loss of: 0.00028610065579414367\n",
      "\n",
      "\n",
      "Epoch: 1243  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1243  | Validation Accuracy 0.0  | Validation MRR: 0.0011261261261261261\n",
      "Epoch: 1243 with a loss of: 0.001972300410270691\n",
      "\n",
      "\n",
      "Epoch: 1244  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0060790273556231\n",
      "Epoch: 1244  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1244 with a loss of: 0.0008767704712226987\n",
      "\n",
      "\n",
      "Epoch: 1245  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0070921985815602835\n",
      "Epoch: 1245  | Validation Accuracy 0.0  | Validation MRR: 0.004004004004004004\n",
      "Epoch: 1245 with a loss of: 8.14095139503479e-06\n",
      "\n",
      "\n",
      "Epoch: 1246  | Training Accuracy 0.005319148936170213  | Training MRR: 0.0060790273556231\n",
      "Epoch: 1246  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.010510510510510511\n",
      "Epoch: 1246 with a loss of: 8.329376578330994e-06\n",
      "\n",
      "\n",
      "Epoch: 1247  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1247  | Validation Accuracy 0.0  | Validation MRR: 0.0032532532532532532\n",
      "Epoch: 1247 with a loss of: 4.002399742603302e-05\n",
      "\n",
      "\n",
      "Epoch: 1248  | Training Accuracy 0.005319148936170213  | Training MRR: 0.00591016548463357\n",
      "Epoch: 1248  | Validation Accuracy 0.0  | Validation MRR: 0.002802802802802803\n",
      "Epoch: 1248 with a loss of: 3.059186041355133e-05\n",
      "\n",
      "\n",
      "Epoch: 1249  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005984042553191489\n",
      "Epoch: 1249  | Validation Accuracy 0.0  | Validation MRR: 0.001287001287001287\n",
      "Epoch: 1249 with a loss of: 6.2047690153121946e-06\n",
      "\n",
      "\n",
      "Epoch: 1250  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005984042553191489\n",
      "Epoch: 1250  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1250 with a loss of: 9.079873561859131e-06\n",
      "\n",
      "\n",
      "Epoch: 1251  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006382978723404255\n",
      "Epoch: 1251  | Validation Accuracy 0.0  | Validation MRR: 0.001287001287001287\n",
      "Epoch: 1251 with a loss of: 4.798471927642822e-06\n",
      "\n",
      "\n",
      "Epoch: 1252  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006870567375886525\n",
      "Epoch: 1252  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1252 with a loss of: 2.8386712074279785e-06\n",
      "\n",
      "\n",
      "Epoch: 1253  | Training Accuracy 0.010638297872340425  | Training MRR: 0.011398176291793313\n",
      "Epoch: 1253  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1253 with a loss of: 2.222582697868347e-06\n",
      "\n",
      "\n",
      "Epoch: 1254  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006382978723404255\n",
      "Epoch: 1254  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1254 with a loss of: 6.651654839515686e-06\n",
      "\n",
      "\n",
      "Epoch: 1255  | Training Accuracy 0.005319148936170213  | Training MRR: 0.008020938872002701\n",
      "Epoch: 1255  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1255 with a loss of: 1.842305064201355e-06\n",
      "\n",
      "\n",
      "Epoch: 1256  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007313829787234043\n",
      "Epoch: 1256  | Validation Accuracy 0.0  | Validation MRR: 0.002927927927927928\n",
      "Epoch: 1256 with a loss of: 0.0008873612433671951\n",
      "\n",
      "\n",
      "Epoch: 1257  | Training Accuracy 0.010638297872340425  | Training MRR: 0.010638297872340425\n",
      "Epoch: 1257  | Validation Accuracy 0.0  | Validation MRR: 0.00429000429000429\n",
      "Epoch: 1257 with a loss of: 1.1011287569999695e-05\n",
      "\n",
      "\n",
      "Epoch: 1258  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1258  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1258 with a loss of: 7.663145661354065e-06\n",
      "\n",
      "\n",
      "Epoch: 1259  | Training Accuracy 0.005319148936170213  | Training MRR: 0.011303191489361703\n",
      "Epoch: 1259  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1259 with a loss of: 3.858447074890137e-05\n",
      "\n",
      "\n",
      "Epoch: 1260  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1260  | Validation Accuracy 0.0  | Validation MRR: 0.0022522522522522522\n",
      "Epoch: 1260 with a loss of: 6.383955478668213e-06\n",
      "\n",
      "\n",
      "Epoch: 1261  | Training Accuracy 0.005319148936170213  | Training MRR: 0.009633569739952719\n",
      "Epoch: 1261  | Validation Accuracy 0.0  | Validation MRR: 0.0015015015015015015\n",
      "Epoch: 1261 with a loss of: 3.5648047924041747e-06\n",
      "\n",
      "\n",
      "Epoch: 1262  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006382978723404255\n",
      "Epoch: 1262  | Validation Accuracy 0.0  | Validation MRR: 0.005255255255255255\n",
      "Epoch: 1262 with a loss of: 5.182474851608277e-06\n",
      "\n",
      "\n",
      "Epoch: 1263  | Training Accuracy 0.005319148936170213  | Training MRR: 0.006648936170212766\n",
      "Epoch: 1263  | Validation Accuracy 0.0  | Validation MRR: 0.0\n",
      "Epoch: 1263 with a loss of: 2.3777633905410767e-05\n",
      "\n",
      "\n",
      "Epoch: 1264  | Training Accuracy 0.0  | Training MRR: 0.0026595744680851063\n",
      "Epoch: 1264  | Validation Accuracy 0.0  | Validation MRR: 0.0022880022880022877\n",
      "Epoch: 1264 with a loss of: 0.0014926812332123518\n",
      "\n",
      "\n",
      "Epoch: 1265  | Training Accuracy 0.010638297872340425  | Training MRR: 0.012854609929078015\n",
      "Epoch: 1265  | Validation Accuracy 0.009009009009009009  | Validation MRR: 0.009009009009009009\n",
      "Epoch: 1265 with a loss of: 2.0537525415420532e-05\n",
      "\n",
      "\n",
      "Epoch: 1266  | Training Accuracy 0.005319148936170213  | Training MRR: 0.005319148936170213\n",
      "Epoch: 1266  | Validation Accuracy 0.0  | Validation MRR: 0.0022522522522522522\n",
      "Epoch: 1266 with a loss of: 0.0011060265451669693\n",
      "\n",
      "\n",
      "Epoch: 1267  | Training Accuracy 0.005319148936170213  | Training MRR: 0.007978723404255319\n",
      "Epoch: 1267  | Validation Accuracy 0.0  | Validation MRR: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import *\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from torch.optim import Adam\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "import numpy as np\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "\"\"\"\n",
    "Possible solutions \n",
    "1) Use larger samples \n",
    "2) Batch it \n",
    "3) Look at other people's articles/publications \n",
    "4) BERT Large \n",
    "\n",
    "1) Inspect by plotting them \n",
    "2) Visually compare the data points \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Things to change\n",
    "if underfitting \n",
    "1) lr \n",
    "2) number of hidden dimensions\n",
    "3) number of hidden layers\n",
    "4) ...\n",
    "\n",
    "if overfitting \n",
    "1) weight decay\n",
    "2) dropout\n",
    "3) ...\n",
    "\n",
    "\"\"\"\n",
    "torch.manual_seed(487)\n",
    "start_epoch = \"1169\"\n",
    "lr = 3e-5\n",
    "hd = 512\n",
    "wd = 0.000001\n",
    "dropout=0.05\n",
    "model = FineTunedModel(hd, dropout=dropout)\n",
    "# pathname = \"save\\\\hd\" + str(hd) + \"_lr\" + str(lr) + \"_wd\" + str(wd) + \"_dropout\" + str(dropout) + \"_epoch\"\n",
    "pathname = \"save\\\\hd\" + str(hd) + \"_lr\" + str(lr) + \"_wd\" + str(wd) + \"_epoch\"\n",
    "\n",
    "\"\"\" !!! LOAD MODEL HERE !!!! \"\"\"\n",
    "# filepath \n",
    "print(\"Loading model...\")\n",
    "model.load_state_dict(torch.load(pathname + start_epoch + \".pth\"))\n",
    "print(\"Model loaded successfully\")\n",
    "\"\"\" !!! LOAD MODEL HERE !!!! \"\"\"\n",
    "\n",
    "optim = Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "criterion = CosineEmbeddingLoss(reduction=\"mean\") # cosine similarity \n",
    "# start the training process \n",
    "num_epoch = 5000\n",
    "\n",
    "# TODO: do the actual data loading \n",
    "\n",
    "def performance(cur_samples, data_partition, k = 10):\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=10, metric=\"cosine\", n_jobs=-1, leaf_size=300)\n",
    "    knn.fit(cur_samples)\n",
    "    mrr = 0\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for entry in data_partition:\n",
    "        # if the label is 0, we don't need to check \n",
    "        if entry[\"label\"] == -1:\n",
    "            continue\n",
    "        num_samples += 1\n",
    "        # first get the new embedding using our model\n",
    "        # We use Q2 because Q2 is the duplicate \n",
    "        # cur_embed = model.ff(entry[\"q1_embedding\"])\n",
    "        cur_embed = model.ff(entry[\"Q1_title\"])\n",
    "        # we also need to get the real id \n",
    "        # original_label = entry[\"q2_id\"]\n",
    "        original_label = entry[\"Q2_id\"]\n",
    "        \n",
    "        nearest_neighbors = knn.kneighbors(cur_embed.detach().reshape(1, -1), k, return_distance=False).tolist()[0]\n",
    "        \n",
    "        # we need to start at the second index, because the first index is always gonan be itself \n",
    "        \n",
    "        # acc\n",
    "        if data.unique_embeddings_id[nearest_neighbors[1]] == original_label: \n",
    "            acc += 1\n",
    "            \n",
    "        # mrr\n",
    "        for i, cur_neighbor in enumerate(nearest_neighbors[1:]):\n",
    "            \n",
    "            if data.unique_embeddings_id[cur_neighbor] == original_label:\n",
    "                mrr += 1 / (1 + i)\n",
    "                break\n",
    "    \n",
    "    mrr /= num_samples\n",
    "    acc /= num_samples\n",
    "    return acc, mrr\n",
    "\n",
    "k = 10\n",
    "best_loss = float(\"infinity\")\n",
    "best_acc = 0\n",
    "best_mrr = 0 \n",
    "best_epoch = 0\n",
    "stats = {\n",
    "    \"best_loss\": float(\"infinity\"),\n",
    "    \"best_loss_ep\": 0,\n",
    "    \"best_vacc\": 0,\n",
    "    \"best_vacc_ep\": 0,\n",
    "    \"best_tacc\": 0,\n",
    "    \"best_tacc_ep\": 0,\n",
    "    \"best_vmrr\": 0,\n",
    "    \"best_vmrr_ep\": 0,\n",
    "    \"best_tmrr\": 0,\n",
    "    \"best_tmrr_ep\": 0\n",
    "}\n",
    "\n",
    "def update_stats(key, value, ep):\n",
    "    \n",
    "    if value > stats[key]:\n",
    "        stats[key+\"_ep\"] = ep \n",
    "        stats[key] = value \n",
    "        \n",
    "# training loop \n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # before we start our loop, we need to retrain our nearest neighbors\n",
    "    cur_samples =[] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for unique_title in data.unique_embeddings:\n",
    "            cur_samples.append(np.array(model.ff(unique_title)))\n",
    "    # print(\"Cur sample first index\", cur_samples[0])\n",
    "    \n",
    "    # now calcualte the mrr and accuracy\n",
    "    vacc, vmrr = performance(cur_samples, X_val)\n",
    "    tacc, tmrr = performance(cur_samples, X_train)\n",
    "    print(\"Epoch:\", epoch+1+int(start_epoch), \" | Training Accuracy\", tacc, \" | Training MRR:\", tmrr)\n",
    "    print(\"Epoch:\", epoch+1+int(start_epoch), \" | Validation Accuracy\", vacc, \" | Validation MRR:\", vmrr)\n",
    "\n",
    "    update_stats(\"best_vacc\", vacc, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_tacc\", tacc, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_vmrr\", vmrr, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_tmrr\", tmrr, epoch+1+int(start_epoch))\n",
    "    \n",
    "    # now start training process \n",
    "    total_loss = 0 \n",
    "    for entry in X_train:   \n",
    "        # q1 = entry[\"q1_embedding\"]\n",
    "        # q2 = entry[\"q2_embedding\"]\n",
    "        q1 = entry[\"Q1_title\"]\n",
    "        q2 = entry[\"Q2_title\"]\n",
    "        y = entry[\"label\"]\n",
    "        \n",
    "        optim.zero_grad()   \n",
    "        # this output a \"new embedding\" for both q1 and q2 \n",
    "        out1, out2, sim = model(q1, q2)\n",
    "        # print(\"Epoch:\", epoch + 1, \" | Similarity between q1 and q2 is:\", sim, \" | True label:\", y)\n",
    "        loss = criterion(out1, out2, torch.tensor(y, dtype=torch.float))\n",
    "        loss.backward()\n",
    "        \n",
    "        if loss < stats[\"best_loss\"]:\n",
    "            stats[\"best_loss_ep\"] = epoch+1+int(start_epoch) \n",
    "            stats[best_loss] = loss\n",
    "            \n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    try:\n",
    "        torch.save(model.state_dict(), pathname + str(epoch + int(start_epoch) + 1) +\".pth\")\n",
    "    except OSError as err:\n",
    "        print(\"An error occured while saving model:\")\n",
    "        print(err)\n",
    "        \n",
    "    print(\"Epoch:\", epoch+1+int(start_epoch), \"with a loss of:\", total_loss/len(data))\n",
    "    \n",
    "    # before we start our loop, we need to retrain our nearest neighbors\n",
    "    cur_samples =[] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for unique_embedding in data.unique_embeddings:\n",
    "            cur_samples.append(np.array(model.ff(unique_embedding)))\n",
    "    # print(\"Cur sample first index\", cur_samples[0])\n",
    "    # now calcualte the mrr and accuracy\n",
    "    vacc, vmrr = performance(cur_samples, X_val)\n",
    "    tacc, tmrr = performance(cur_samples, X_train)\n",
    "    print(\"Epoch:\", epoch+1+int(start_epoch), \" | Training Accuracy\", tacc, \" | Training MRR:\", tmrr)\n",
    "    print(\"Epoch:\", epoch+1+int(start_epoch), \" | Validation Accuracy\", vacc, \" | Validation MRR:\", vmrr)\n",
    "\n",
    "    update_stats(\"best_vacc\", vacc, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_tacc\", tacc, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_vmrr\", vmrr, epoch+1+int(start_epoch))\n",
    "    update_stats(\"best_tmrr\", tmrr, epoch+1+int(start_epoch))\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.0  | Training MRR: 0.005074299223235393\n",
      "Validation Accuracy 0.0  | Validation MRR: 0.004129129129129129\n"
     ]
    }
   ],
   "source": [
    "# get the baseline metric \n",
    "\n",
    "from model import *\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from torch.optim import Adam\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "import numpy as np\n",
    "from torch.nn.functional import normalize\n",
    "torch.manual_seed(487)\n",
    "def performance_no_finetune(cur_samples, data_partition, k = 10):\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=10, metric=\"cosine\", n_jobs=-1, leaf_size=300)\n",
    "    knn.fit(cur_samples)\n",
    "    mrr = 0\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for entry in data_partition:\n",
    "        # if the label is 0, we don't need to check \n",
    "        if entry[\"label\"] == -1:\n",
    "            continue\n",
    "        num_samples += 1\n",
    "        # first get the new embedding using our model\n",
    "        # We use Q2 because Q2 is the duplicate \n",
    "        # cur_embed = model.ff(entry[\"Q1_title\"])\n",
    "        cur_embed = entry[\"Q1_title\"]\n",
    "        # we also need to get the real id \n",
    "        original_label = entry[\"Q2_id\"]\n",
    "        \n",
    "        nearest_neighbors = knn.kneighbors(cur_embed.detach().reshape(1, -1), k, return_distance=False).tolist()[0]\n",
    "        \n",
    "        # we need to start at the second index, because the first index is always gonan be itself \n",
    "        \n",
    "        # acc\n",
    "        if data.unique_embeddings_id[nearest_neighbors[1]] == original_label: \n",
    "            acc += 1\n",
    "            \n",
    "        # mrr\n",
    "        for i, cur_neighbor in enumerate(nearest_neighbors[1:]):\n",
    "            \n",
    "            if data.unique_embeddings_id[cur_neighbor] == original_label:\n",
    "                mrr += 1 / (1 + i)\n",
    "                break\n",
    "    \n",
    "    mrr /= num_samples\n",
    "    acc /= num_samples\n",
    "    return acc, mrr\n",
    "cur_samples = []\n",
    "\n",
    "for entry in data.data:\n",
    "    cur_samples.append(np.array(entry[\"Q1_title\"]))\n",
    "    \n",
    "vacc, vmrr = performance_no_finetune(cur_samples, X_val)\n",
    "tacc, tmrr = performance_no_finetune(cur_samples, X_train)\n",
    "print(\"Training Accuracy\", tacc, \" | Training MRR:\", tmrr)\n",
    "print(\"Validation Accuracy\", vacc, \" | Validation MRR:\", vmrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
